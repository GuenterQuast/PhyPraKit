{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Jupyter-Notebook-Tutorial:   \n",
    "#    Anpassung von Modellen an Daten mit phyFit \n",
    "\n",
    "                                                Günter Quast, Juni 2021\n",
    "---\n",
    "## Grundsätzliches zu Jupyter Notebooks\n",
    "\n",
    "Diese Datei vom Typ `.ipynb` enthält ein Tutorial als `Jupyter notebook`.\n",
    "*Jupyter* bietet eine Browser-Schnittstelle mit einer (einfachen) Entwicklungsumgebung\n",
    "für *Python*-Code und erklärende Texte im intuitiven *Markdown*-Format.\n",
    "Die Eingabe von Formeln im *LaTeX*-Format wird ebenfalls unterstützt.\n",
    "\n",
    "Eine Zusammenstellung der wichtigsten Befehle zur Verwendung von *Jupyter* als Arbeitsumgebung\n",
    "findet sich im Notebook\n",
    "[*JupyterCheatsheet.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/JupyterCheatsheet.ipynb).\n",
    "Grundlagen zur statistischen Datenauswertung finden sich in den Notebooks \n",
    "[*IntroStatistik.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/IntroStatistik.ipynb)\n",
    "und\n",
    "[*Fehlerrechnung.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/Fehlerrechnung.ipynb).\n",
    "\n",
    "In *Jupyter* werden Code und Text in jeweils einzelne Zellen eingegeben. \n",
    "Aktive Zellen werden durch einen blauen Balken am Rand angezeigt.\n",
    "Sie können sich in zwei Zuständen befinden: im Edit-Mode ist das Eingabefeld weiß, im\n",
    "Command-Mode ist es ausgegraut.\n",
    "Durch Klicken in den Randbereich wird der Command-Mode gewählt, ein Klick in das Textfeld einer\n",
    "Code-Zelle schaltet in den Edit-Mode.\n",
    "Die Taste `esc` kann ebenfalls verwendet werden, um den Edit-Mode zu verlassen.\n",
    "\n",
    "Die Eingabe von `a` im Command-Mode erzeugt eine neue leere Zelle oberhalb der aktiven Zelle, `b`\n",
    "eine unterhalb. Eingabe von `dd` löscht die betreffende Zelle.\n",
    "\n",
    "Zellen können entweder den Typ `Markdown` oder `Code` haben.\n",
    "Die Eingabe von `m` im Command-Mode setzt den Typ Markdown, Eingabe von `y` wählt den Typ Code.\n",
    "\n",
    "Prozessiert - also Text gesetzt oder Code ausgeführt - wird der Zelleninhalt durch Eingabe von\n",
    "`shift+return`, oder auch `alt+return` wenn zusätzlich eine neue, leere Zelle erzeugt werden soll.\n",
    "\n",
    "Die hier genannten Einstellungen sowie das Einfügen, Löschen oder Ausführen von Zellen sind\n",
    "auch über das PullDown-Menü am oberen Rand verfügbar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anmerkungen zur Anpassung von Modellen an Messdaten\n",
    "***\n",
    "Die Physik als \"Theorie-geleitete Erfahrungswissenschaft\" lebt vom\n",
    "Zusammenspiel theoretischer Hypothesenbildung und deren stringenter\n",
    "Überprüfung im Experiment. Bei der Interpretation von Messdaten kommt\n",
    "es daher zunächst darauf an, ob sie überhaupt einem erwarteten Modell \n",
    "entsprechen. Erst, wenn diese Frage mit \"ja\" beantwortet werden kann, \n",
    "ist die Extraktion vom Modellparametern sinnvoll.\n",
    "\n",
    "Leider tragen die meisten der häufig verwendeten Werkzeuge  in der \n",
    "Voreinstellung dieser Anforderung nicht Rechnung, sondern zielen \n",
    "auf eine möglichst genaue Parametrisierung der Daten ab. \n",
    "Bei Vorgabe von unpassenden Modellen werden dazu die Parameterfehler\n",
    "so weit vergrößert, dass die Parametrisierung möglichst genau passt.\n",
    "\n",
    "Um den besonderen Bedingungen der Datenauswertung in der Physik\n",
    "gerecht zu werden, ist die Verwendung von eigenem Programmcode, \n",
    "idealerweise in der Programmiersprache {\\em Python} in einem \n",
    "Jupyter-Notebook, häufig notwendig, da die meisten mittels einer\n",
    "grafischen Oberfläche zu bedienenden Werkzeuge zur statistischen\n",
    "Datenauswertung den Anforderungen nicht oder nur zum Teil genügen. \n",
    "\n",
    "Wichtige Anforedrugnen an eine Methode zur Modellanpassung sind:\n",
    "\n",
    "- Alle Messunsicherheiten sollten direkt bei der Modellanpassung berücksichtigt\n",
    "  und entsprechend modelliert werden, um sie vollständig und korrekt auf die\n",
    "  Unsicherheiten der Parameter zu propagieren. Eine später von Hand\n",
    "  durchgeführte Fehlerfortpflanzung ist aufwändig und funktioniert nur\n",
    "  in einfachsten Fällen.   \n",
    "- Häufig treten auch relative Unsicherheiten auf, z.$\\,$B. bei\n",
    "  Kalibrationsunsicherheiten, die als Bruchteil des wahren Messwertes\n",
    "  auftreten. Wenn solche relative Unsicherheiten korrekt behandelt werden\n",
    "  sollen, geht das mit der Methode der kleinsten Fehlerquadrate nur noch\n",
    "  näherungsweise. Exakte Parameterschätzungen beruhen auf der\n",
    "  Maximum-Likelihood-Methode (`MLE', Maximum Likelihood Estimation).\n",
    "- In der Quanten-, Kern- und Teilchenphysik treten als Messgrößen\n",
    "  Zählraten in Abhängigkeit von Energie, Streuwinkel oder anderen\n",
    "  Größen auf, die üblicherweise als Häufig\\-keits\\-ver\\-teilung (Histogramm)\n",
    "  dargestellt werden. Die Einträge in einem Intervall eines Histogramms\n",
    "  (einem sog. `Bin`) sind aber nicht Gauß-, sondern Poisson-verteilt. Auch\n",
    "  näherungsweise können Bins mit Null Einträgen mit einer - oft immer\n",
    "  noch verwendeten - angepassten Methode der kleinsten Fehlerquadrate\n",
    "  nicht berücksichtigt werden. Zur Anpassung von Modellen an Histogramme\n",
    "  müssen also ebenfalls maximum-likelihood Methoden angewendet werden.\n",
    "- Bei der Analyse von sehr kleinen Zählraten bei nur selten eintretenden\n",
    "  Ereignissen kann schon die Notwendigkeit zur Aufteilung in Bins zur einem\n",
    "  Informationsverlust und zu einer Verzerrung des Ergebnisses führen.\n",
    "  In solchen Fällen sollte ein ungebinnter Maximum-Likelihod-Fit zur\n",
    "  Anwendung kommen.\n",
    "- Bei der Anpassung an zweidimensonale Datenppunkte $(x_i, y_i)$ müssen \n",
    "  die Werkzeuge Unsicherheiten in Abszissenrichtung zusätzlich zu denen in \n",
    "  Ordinatenrichtung behandeln können. Die Unterstützung von korrelierten \n",
    "  Unsicherheiten ist ebenfalls notwendig, um typische Charakteristika von \n",
    "  Messgeräten, wie korrelierte Kalibrationsunsicherheiten zusätzlich zu den\n",
    "  unabhängigen Digitalisierungsunsicherheiten und Rauschbeiträgen,\n",
    "  darstellen zu können.\n",
    "\n",
    "Leider gibt es kaum Anpassungswerkzeuge, die alle genannten Anforderungen\n",
    "gleichzeitig erfüllen. Deshalb wurde am ETP in zahlreichen Bachelor-Arbeiten\n",
    "ein quelloffenes Anpassungswerkzeug, [kafe2](https://github.com/dsavoiu/kafe2)\n",
    "entwickelt, das die genannten Aspekte abdeckt. Allerdings ist der Programmcode\n",
    "wegen der großen Zahl an Optionen und Methoden sehr komplex und für Einsteiger unübersichtlich.\n",
    "\n",
    "Zur Illustration der Vorgehensweise wird daher in diesem Tutorial ein\n",
    "einziges, sehr flexibles Werkzeug zur numerischen Optimierung und \n",
    "Analyse der Parameterunsicherheiten verwendet, nämlich das am CERN \n",
    "entwickelte Paket `Minuit` bzw. dessen Python-Interface \n",
    "[iminuit](https://iminuit.readthedocs.io/en/stable/). \n",
    "Eine schlanke Implementierung von Anwendungsbeispielen bietet das Paket \n",
    "[PhyPraKit.phyFit](https://phyprakit.readthedocs.io/en/latest/), das \n",
    "hier verwendet wird, um die Grundlagen der Anpassung von Modellen an \n",
    "Messdaten mit der Maximum-Likelihood Methode darzustellen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematische Grundlagen der Parameterschätzung\n",
    "***\n",
    "\n",
    "In diesem Tutorial werden ausgehend vom Maximum-Likelihood-Prinzip \n",
    "zunächst typische Anwendung zur Anpassung von Verteilungsdichten an\n",
    "Messdaten diskutiert und dann die Verbindung zur altbekannten Methode\n",
    "der kleinsten Quadrate zur Anpassung von parameterbehafteten \n",
    "Modellfunktionen $y_i = f(x_i; *par )$ an zweidimensionale Datenpunkte\n",
    "$(x_i, y_i)$ dargestellt. \n",
    "\n",
    "Ausgangspunkt der Überlegungen ist der auf der Maximum-Likeliood Methode\n",
    "basierende Formalismus zur Parameterschätung von Verteilungsdichten, \n",
    "die den Messergebnissen zu Grunde liegen. Manchmal sind die Parameter der \n",
    "Verteilungsdichte selbst interessant, z.$\\,$B. bei der Lebensdauerverteilung \n",
    "von quantenmechanischen Zuständen oder von (Elementar-)Teilchen.\n",
    "Häufig treten Parameter von Verteilungsdichten aber auch nur als \n",
    "`Störparameter` auf, die statistische Fluktuationen um einen angenommenen\n",
    "wahren Wert beschreiben. Die Behandlung von Messunsicherheiten von \n",
    "Datenpunkten $(x_i, y_i)$ ist dafür ein Beispiel. \n",
    "\n",
    "\n",
    "## Parameterschätzug mit dem Maximum-Likelihood Verfahren  \n",
    "\n",
    "Beginnen wir an dieser Stelle mit der einfachsten Problemstellung, \n",
    "der Schätung der Parameter der Verteilungsdichte einer Grundgesamtheit,\n",
    "aus eine endliche Stichprobe (=\"Daten\") gezogen werden. \n",
    "\n",
    "Wir gehen von einer Verteilungsdichte ${\\rm pdf}(x, \\vec p)$ aus,\n",
    "die von einer Anzahl von Parametern $p_j$ abhängt. Ein Datensatz\n",
    "$\\vec x$ besteht aus ${n_d}$ voneinander unabhängigen\n",
    "Einzelmessungen $x_i$.\n",
    "\n",
    "Nach dem Maximum Likelihood-Prinzip ist der beste Schätzwert der\n",
    "Parameter ${\\hat{ \\vec {p}}}$ gegeben durch die Werte, die die\n",
    "sog. `Likelihood`, das Produkt der Werte der $pdf$ für die\n",
    "Einzelmessungen, ${\\rm pdf}(x_i, \\vec p)$, maximieren:\n",
    "\n",
    "\\begin{equation}\\label{equ:Likelihood1}\n",
    "  {\\cal L}(\\vec x, \\vec p) =\n",
    "  \\displaystyle\\prod_{i=1}^{n_d} \\, {\\rm pdf}(x_i, \\vec p) \\,.\n",
    "\\end{equation}  \n",
    "\n",
    "Um unhandlich kleine Werte des Produkts zu vermeiden, aus Gründen\n",
    "der numerischen Stabilität und auch aus mathematischen Gründen\n",
    "(s. \"Fisher-Information\") verwendet man den negativen Logarithmus\n",
    "von $\\cal L$, die `negative log-Likelihood` $nl\\cal L$, gegeben durch\n",
    "\\begin{equation}\\label{equ:Likelihood2}\n",
    "  {nl\\cal L}(\\vec x, \\vec p) =\n",
    "  -\\displaystyle\\sum_{i=1}^{n_d} \\, \\ln\\left({\\rm pdf}(x_i, \\vec p)\\right) \\,.\n",
    "\\end{equation}  \n",
    "\n",
    "Die negative log-Likelihood-Funktion wird dann als Funktion der Parameter\n",
    "aufgefasst und bezüglich der Parameter minimiert. Üblicherweise verwendet\n",
    "man sie als Kostenfunktion in numerischen Optimierungsverfahren.\n",
    "\n",
    "In der Physik ist es auch üblich, einen mit Zwei multiplizieten Wert zu verwenden, \n",
    "${n2l\\cal L}(\\vec x, \\vec p) = 2 \\cdot {nl\\cal L}(\\vec x, \\vec p)$.\n",
    "Wie wir unten sehen werden, entpricht dies in Spezialfällen dem Wert der\n",
    "Residuensumme, $S$, der nach dem von Gauß vorgeschlagenen Verfahren der \n",
    "kleinsten Fehlerquadrate häufig als Kostenfunktion verwendet wird. \n",
    "\n",
    "Es lässt sich zeigen, dass das log-Likelihood-Verfahren unter allen \n",
    "Verfahren zur Parameterschätzung optimal ist, also die kleinste Varianz \n",
    "der Parameterwerte liefert. Auch dies ist ein Grund, dem Likelihood-Verfahren\n",
    "den Vorzug zu geben, wenn man maximalen Informationsgewinn aus bisweilen\n",
    "extrem aufwändigen und damit teuren Experimenten ziehen möchte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für ein digitales Tutorial ist es an dieser Stelle Zeit für ein konkretes Beispiel.\n",
    "Wir betrachten Daten, an die eine Gauß-Verteilung angepasst wird. \n",
    "\n",
    "Mit Hilfe des folgen Codefragments werden zunächst (Pseudo-)Daten erzeugt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate Gaussian-distributed data\n",
    "mu0=2.\n",
    "sig0=0.5\n",
    "np.random.seed(314159)  # initialize random generator\n",
    "data = mu0 + sig0 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gaussverteilung mit Erwartungswert $\\mu$ und Standardabweichung $\\sigma$ ist  \n",
    "\n",
    "$Gauss(x; \\mu, \\sigma) \\,=\\, \\frac{1}{\\sqrt{2\\pi} \\, \\sigma} \\exp -{\\frac{(x-\\mu)^2}{2 \\sigma^2}}$\n",
    "\n",
    "Das Zweifache des negativen natürlichen Logarithmus der Likelihood davon ist\n",
    "\n",
    "$-2\\, \\ln(Gauss(x; \\mu, \\sigma))\\,=\\, \\left(\\frac{x-\\mu}{\\sigma}\\right)^2 + 2\\ln(\\sigma) + \\ln (2\\pi) $\n",
    "\n",
    "Den von allen Variablen unabhängigen Term $\\ln(2\\pi)$ kann man weglassen, \n",
    "weil er auf die Lage des Minimums keinen Einfluss hat. \n",
    "\n",
    "Die Kostenfunktion $n2l{\\cal L}$ ist die Summe solcher Ausdrücke über alle $n_d$ Werte\n",
    "einer Messreihe:\n",
    "\n",
    "$n2l{\\cal L} = 2 n_d \\ln(\\sigma) + \n",
    "                \\displaystyle \\sum_i^{n_d} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$\n",
    "\n",
    "Der Code dazusieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function: 2 * negative log likelihood of Gauß;\n",
    "def myCost(mu=1., sigma=1.):\n",
    "  # simple -2*log-likelihood of a 1-d Gauss distribution\n",
    "  r = (data-mu)/sigma\n",
    "  return np.sum( r*r) + 2.*len(data)*np.log(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obwohl in diesem Fall eine analytische Minimierung der Kostenfunktion möglich wäre, wollen wir eine numerische Minimierung verwenden. Verwendet wird im Beispiel in der Code-Zelle unten das Paket MINUIT mit Hilfe des *Python*-basiertes Interfaces *iminuit*. \n",
    "Neben der Minimierung einer skaleren, von Parametern abhängigen \"Kostenfunktion\" bietet MINUIT auch umfanreiche Methoden zur Steuerung der Anpassung und zur Analyse der Parameterunsicherheiten. \n",
    "Für unser einfaches Problem sieht der *iminuit*-Code so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for iminuit vers. >2.0\n",
    "from iminuit import Minuit\n",
    "\n",
    "# initialize Minuit object\n",
    "m = Minuit(myCost, mu=1., sigma=1.)\n",
    "m.errordef = 1.         # internal parameter, needed to control uncertainty analysis\n",
    "\n",
    "# perform optimzation\n",
    "m.migrad()\n",
    "\n",
    "# print results\n",
    "print(\"parameter names: \", m.parameters)\n",
    "print(\"best-fit values: \", m.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas einfacher wird die Anpassung mit zuätzlicher Software, die *Minuit* und das mächtige, aber damit auch notwendigerweise komplizierte Interface kapselt und so typische Anwendungsfälle vereinfacht, wie z.$\\,$B. das Werkzeug [*kafe2*](https://github.com/dsavoiu/kafe2), oder das\n",
    "schlanke, im Paket *PhyPraKit* enthaltene Paket *phyFit*. Für einfache Anpassungen mit *iminuit*\n",
    "ist die Interface-Funktion `mFit` geeignet. Übergeben wird lediglich die Kostenfunktion; \n",
    "Rückgabewerte sind die Namen und optimalen Parameterwerte, die in der Kostenfunktion als\n",
    "Keyword-Argumente angebeben sind. Zurückgegeben wird weiter eine Schätzung der Parameterunsicherheiten \n",
    "und ein Paramter zur Einschätzung der Qualität der Anpassung. \n",
    "Über diese letztgenannten Größen wir weiter unten noch zu sprechen sein wird.  \n",
    "\n",
    "Das Anpassungsbeispiel mit *mFit* sieht wie folgt aus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhyPraKit.phyFit import mFit  \n",
    "\n",
    "pnams, pvals, perrs, cor, gof = mFit(myCost)\n",
    "\n",
    "# Print results\n",
    "print('\\n*==* user-defined cost: Fit Result:')\n",
    "print(\" parameter names:       \", pnams)\n",
    "print(\" parameter values:      \", pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe zeigt, dass die so bestimmten Parameterwerte den für die Erzeugung\n",
    "der Pseudo-Daten verwendeten Paramtern entspricht. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bestimmung der Parameterunsicherheiten und Intervallschätzung\n",
    "\n",
    "Neben den besten Werten aller Parameter sind ebenso deren Unsicherheiten\n",
    "bedeutsam. Die Varianz der Parameter, oder besser die Kovarianzmatrix\n",
    "der Unsicherheiten aller Parameter, lässt sich aus dem Verlauf der\n",
    "log-Likelihood am Minimum bestimmen. Anschaulich ist ein Parameter\n",
    "um so genauer bestimmt, je \"schärfer\" das Minimum ausgeprägt ist;\n",
    "dies kann über die zweiten Ableitungen der log-Likelihood nach den\n",
    "Parametern quantifiziert werden: \n",
    "\n",
    "\\begin{equation}\\label{equ-Vij}\n",
    "{V_{ij}}^{-1}={\\left. \n",
    "\\frac {{\\partial}^2 \\ln{\\cal{L}}()}\n",
    "{\\partial p_i \\, \\partial p_j} \n",
    "\\right|}_{\\hat p_i \\hat p_j}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Die Matrix $V_{ij}$ ist die Kovarianzmatrix der Parameterunsicherheiten, die wichtig ist,\n",
    "weil Korrelationen zwischen den Parametern auch bei unkorrelierten Messwerten sehr häufig\n",
    "sind. Wenn Korrelationen besonders groß sind, kann dies auch ein Hinweis auf eine ungeschickte\n",
    "Parametrisierung des Problems sein. Bei Korrelationen größer als 10% sollten sie als Teil des\n",
    "Ergebnisses unbedingt kommuniziert werden, weil die Form der innerhalb eines vorgegebenen\n",
    "Konfidenzniveaus noch zulässigen Parameterbereiche davon abhängt. \n",
    "\n",
    "Bei nichtlinearen Problemstellungen ist der Verlauf der log-Likelihood am Minimum\n",
    "nicht parabolisch. In solchen Fällen ist die Angabe eines aud den zweiten Ableitungen\n",
    "am Punkt des Minimums beruhenden symmetrischen Intervalls für die Unsicherheiten nicht \n",
    "ausreichend. Dann verwendet man zur Abschätzung des Vertrauensintervalls der Paramter \n",
    "einen Scan der sog. Profil-Likelihood.\n",
    "Bei diesem Verfahren wird für jeden Parameter eine Anzahl von dicht in de Nähe des\n",
    "Minimums liegenden Punkten betrachtet und die log-Likelihood bezüglich aller anderen \n",
    "Parameter minimiert. Auf diese Weise werden Korrelationen mit anderen Parametern\n",
    "berücksichtigt. Das Vertrauensintervall, das dem Bereich plus-minus einer\n",
    "Standardabweichung einer Gaußverteilung um den Parameterwert, also\n",
    "einem Konfidenzniveau von 68.3% entspricht, erhält man an den Stellen,\n",
    "an denen die log-Likelihood um den Wert $\\frac{1}{2}$ größer ist als\n",
    "am Minimum. Für nicht-parabolische Verläufe ergibt sich aus der Analyse\n",
    "der Profil-Likelihood ein asymmetrisches Konfidenzintervall um den\n",
    "Parameterwert am Minimum, das in solchen Fällen an Stelle der aus den zweiten \n",
    "Ableitungen am Minimum bestimmen Schätzungen der Standardabweichungen der Parameter\n",
    "angegeben werden sollte.\n",
    "\n",
    "Das Verfahren der Profil-Likelihood lässt sich auch in zwei Dimensionen,\n",
    "also für Paare von Parametern, durchführen. Man erhält dann Konturlinien,\n",
    "die Konfidenzbereichen analog zu ein- oder auch zwei-$\\sigma$-Konturen von \n",
    "Gauß-förmigen Verteilungen entsprechen. Bei starker Abweichung der Konturen \n",
    "von der Ellipsenform sollten sie zusammen mit dem Ergebnis berichtet werden.\n",
    "\n",
    "Profile-Likelihood-Kurven und Konfidenzkonturen lassen sich mit dem oben \n",
    "gezeigten Code leicht erzeugen, wenn die zusätzliche Option `plot_cor=True`\n",
    "angegeben wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnams, pvals, perrs, cor, gof = mFit(myCost, plot_cor=True)\n",
    "\n",
    "# Print results\n",
    "print('\\n*==* user-defined cost: Fit Result:')\n",
    "print(\" parameter names:       \", pnams)\n",
    "print(\" parameter values:      \", pvals)\n",
    "print(\" neg. parameter errors: \", perrs[:,0])\n",
    "print(\" pos. parameter errors: \", perrs[:,1])\n",
    "print(\" correlations : \\n\", cor)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-LikelihoodFit: Beispiel 2\n",
    "\n",
    "Wir wollen nun ein etwas komplexeres, realistisches Beispiel zur Anpassung von\n",
    "Parameteren and Daten mit Hilfe des Maximum-Likelihood Verfahrens anschauen. \n",
    "Als Daten verwenden wir Messugnen der Lebensdauern von gestoppten \n",
    "kosmischen Myonen, die in einem Wasser-Cherenkov-Detektor (in diesem Fall \n",
    "einer Kaffeekanne mit Photomultiplier) nachgewiesen wurden. Beim Durchgang \n",
    "durch das Wasser erzeugen die Myonen ein Lichtsignal; wenn sie im Wasser\n",
    "oder im Boden darunter gestoppt werden, können Elektronen aus den zerfallenden\n",
    "Myonen wieder in den Detektor gelangen und nachgewiesen werden. Die Daten\n",
    "entsprechen den Zeitdifferenzen (in µs) solcher Doppelpulse. Zufällig\n",
    "eintreffende Myonen und Detektoruntergrund erzeugen ebenfalls Doppelpulse, \n",
    "die im Gegensatz zu den Lebensdauern aber flach verteilt sind und einen \n",
    "Untergrund bei der Lebensdauermessung darstellen.\n",
    "\n",
    "Die relevante, auf Eins normierte Verteilungsdichte und die Daten finden \n",
    "sich in der folgenden Code-Zelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentialDecayPDF(t, tau=2., fbg=0.2, a=1., b=11.5):\n",
    "  \"\"\"Probability density function \n",
    "\n",
    "    for an exponential decay with flat background. The pdf is normed for the interval \n",
    "    [a=1µs,  b=11.5µs); these parameters a and b must be fixed in the fit! \n",
    "  \"\"\"\n",
    "\n",
    "  # 1.) Exponential distribution of life time measuremtens in the intervall [a, b]\n",
    "  pdf1 = np.exp(-t / tau) / tau / (np.exp(-a / tau) - np.exp(-b / tau))\n",
    "\n",
    "  # 2. flat distribution in the interval [a, b]\n",
    "  pdf2 = 1. / (b - a)\n",
    "\n",
    "# the full distribution is a combinatin of both, with a background fraction fbg   \n",
    "  return (1 - fbg) * pdf1 + fbg * pdf2\n",
    "\n",
    "\n",
    "# real data from measurement with a Water Cherenkov detector (\"Kamiokanne\")\n",
    "dT=[7.42, 3.773, 5.968, 4.924, 1.468, 4.664, 1.745, 2.144, 3.836, 3.132,\n",
    "    1.568, 2.352, 2.132, 9.381, 1.484, 1.181, 5.004, 3.06,  4.582, 2.076,\n",
    "    1.88,  1.337, 3.092, 2.265, 1.208, 2.753, 4.457, 3.499, 8.192, 5.101,\n",
    "    1.572, 5.152, 4.181, 3.52,  1.344, 10.29, 1.152, 2.348, 2.228, 2.172,\n",
    "    7.448, 1.108, 4.344, 2.042, 5.088, 1.02,  1.051, 1.987, 1.935, 3.773,\n",
    "    4.092, 1.628, 1.688, 4.502, 4.687, 6.755, 2.56,  1.208, 2.649, 1.012,\n",
    "    1.73,  2.164, 1.728, 4.646, 2.916, 1.101, 2.54,  1.02,  1.176, 4.716,\n",
    "    9.671, 1.692, 9.292, 10.72, 2.164, 2.084, 2.616, 1.584, 5.236, 3.663,\n",
    "    3.624, 1.051, 1.544, 1.496, 1.883, 1.92,  5.968, 5.89,  2.896, 2.76,\n",
    "    1.475, 2.644, 3.6,   5.324, 8.361, 3.052, 7.703, 3.83,  1.444, 1.343,\n",
    "    4.736, 8.7,   6.192, 5.796, 1.4,   3.392, 7.808, 6.344, 1.884, 2.332,\n",
    "    1.76,  4.344, 2.988, 7.44,  5.804, 9.5,   9.904, 3.196, 3.012, 6.056,\n",
    "    6.328, 9.064, 3.068, 9.352, 1.936, 1.08,  1.984, 1.792, 9.384, 10.15,\n",
    "    4.756, 1.52,  3.912, 1.712, 10.57, 5.304, 2.968, 9.632, 7.116, 1.212,\n",
    "    8.532, 3.000, 4.792, 2.512, 1.352, 2.168, 4.344, 1.316, 1.468, 1.152,\n",
    "    6.024, 3.272, 4.96, 10.16,  2.14,  2.856, 10.01, 1.232, 2.668, 9.176 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anpassung mit Hilfe des log-Likelihood-Verfahrens funktioniert genau so \n",
    "wie oben. Dieses Mal werden allerdings an Stelle der Kostenfunktion die Daten\n",
    "und die Verteilungsdichte sowie weitere Optionen zur grafischen Darstellung \n",
    "als Parameter angegeben. Damit die Anpassung funktioniert, muss das\n",
    "Anpassungswerkzeug einige besondere Optionen unterstützen: \n",
    "- die Begrenzung von Parametern auf einen sinnvollen Bereich   \n",
    "  (Option `limits=('fbg', 0., 1.)`) und \n",
    "- die Fixierung von Paramtern, die in der Anpassung nicht variiert werden, \n",
    "  in diesem Fall das Intervall, in dem die Messungen verlässlich sind\n",
    "  und auf das die Verteilungsdiche normiert ist,   \n",
    "  (Option `fixPars = ['a', 'b']`).\n",
    "  \n",
    "Hier nun der vollständige Programmcode zur Anpassung einer Verteilungsdichte\n",
    "an (ungebinnte) Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mFit( exponentialDecayPDF,\n",
    "       data = dT, # data - if not None, a normalised PDF is assumed as model  \n",
    "       limits=('fbg', 0., 1.),  # parameter limits\n",
    "       fixPars = ['a', 'b'],    # fix parameter(s) \n",
    "       neg2logL = True,         # use  -2 * ln(L)\n",
    "       plot=True,               # plot data and model\n",
    "       plot_band=True,          # plot model confidence-band\n",
    "       plot_cor=True,          # plot profiles likelihood and contours\n",
    "       axis_labels=['life time  ' + '$\\Delta$t ($\\mu$s)', \n",
    "                       'Probability Density  pdf($\\Delta$t; *p)'], \n",
    "       data_legend = '$\\mu$ lifetime data',    \n",
    "       model_legend = 'exponential decay + flat background' )\n",
    "# Print results \n",
    "pnams, pvals, perrs, cor, gof = results\n",
    "print('\\n*==* unbinned ML Fit Result:')\n",
    "print(\" parameter names:       \", pnams)\n",
    "print(\" parameter values:      \", pvals)\n",
    "print(\" neg. parameter errors: \", perrs[:,0])\n",
    "print(\" pos. parameter errors: \", perrs[:,1])\n",
    "print(\" correlations : \\n\", cor)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anders als bei der Anpassung der Gaußverteilung an Gauß-verteilte Daten zeigen die Profil-Likelihood\n",
    "und die Konturen in diesem Fall eine starke Abweichung von linearem Verhalten. \n",
    "In einem solchen Fall ist die Bereitstellung der Grafiken zusätzlich zur Angabe der numerischen Ergebnisse notwendig. Auch die stark asymmetrischen Unsicherheiten müssen unbedingt angegeben werden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anpassung von Verteilungsdichten an Histogramme\n",
    "\n",
    "Ein weiteres typisches Beispiel für die Anwendung der log-Likelihood-Methode\n",
    "ist die Behandlung von Problemen, bei denen Poisson-verteilte Größen\n",
    "auftreten. Dies sind z.$\\,$B. die Anzahlen von Einträgen in einzelnen\n",
    "Intervallen von Häufigkeitsverteilungen (Histogramme).\n",
    "\n",
    "Die Poissonverteilung von Anzahlen $n_i$ mit Erwartungswerten $\\mu_i$\n",
    "ist gegeben durch\n",
    "\n",
    "\\begin{equation}\\label{equ-Poisson}\n",
    "  P(n_i;\\mu_i)=\\frac{{\\mu_i}^{n_i}} {n_i\\,!} \\, {\\rm e}^{-\\mu_i} \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Durch Bilden des Produkts über alle (als statistisch unabhängig\n",
    "angenommenen) $n_b$ Bins eines Histogramms erhält man die Likelihood \n",
    "\n",
    "\\begin{equation}\\label{equ-LPoisson}\n",
    "  {\\cal{L}}_{Poisson}=\\displaystyle\\prod_{i=1}^{n_b}\n",
    "  {\\rm P}\\left(n_i;\\mu_i(\\vec p)\\right)\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "und schließlich durch Logarithmieren die log-Likelihood der\n",
    "Poisson-verteilten Anzahlen der Einträge in den Bins eines Histogramms\n",
    "\\begin{equation}\\label{equ-nlLPoisson}\n",
    "nl{\\cal{L}}_{Poisson}= -\\ln {\\cal{L}}_{Poisson} =\n",
    "\\displaystyle\\sum_{i=1}^{n_b} - n_i \\cdot \\ln(\\mu_i(\\vec p))\\,+\\,\\mu_i(\\vec p) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Bei Verwendung dieser Kostenfunktion im Minimierungsprozess können\n",
    "Anpassungen von Verteilungsdichten an Histogramme ganz analog wie bei\n",
    "der Verwendung der oben bereits diskutierten Likelihood zur Anpassung\n",
    "von Verteilungsdichten an Messdaten durchgeführt werden, die in diesem\n",
    "Fall aber als Historgramm vorliegen. \n",
    "Der Rechenaufwand hängt in diesem Beispiel von der Zahl der Bins und\n",
    "nicht - wie im ersten Beispiel - von der Größe des Datensatzes ab. \n",
    "\n",
    "Die Anzahl der auf Grund der Verteilungsdichte erwarteten Einträge $n_i$ \n",
    "bestimmt man sinnvollerweise durch Bildung des Integrals der Verteilung\n",
    "über die Binbreite - eine numerische Näherung (z.B. Simpson 2. Ordnung)\n",
    "ist meist ausreichend. \n",
    "\n",
    "In manchen Fällen wird auch eine Gauß-Näherung der Poisson-Verteilung\n",
    "verwendet,  $P(n_i;\\mu_i) = Gauss(n_i, \\mu_i, \\sigma=\\sqrt{\\mu_i})$.\n",
    "Bei großen Anzahlen an Bin-Einträgen ist diese Näherung aktzeptabel und\n",
    "erlaubt häufig die Anwendung von Programmen zur Anpassung, die auf\n",
    "der Methode der kleinsten Fehlerquadrate beruhen und daher nur eine\n",
    "auf der Gaußverteilung beruhende Kostenfunktion erlauben. \n",
    "\n",
    "Wichtig ist es, die Schäzung der Unsicherheiten $\\sigma_i = \\sqrt{\\mu_i}$\n",
    "aus der Modellerwartung und nicht aus den beobachteten Werten in den\n",
    "Daten zu bestimmen, da letzteres zu einer verzerrten Parameterschätzung\n",
    "führen würde und Bins mit Null Einträgen sogar weggelassen werden müssten. \n",
    "Im Notfall kann man sich behelfen, indem zunächst eine Anpassung mit den aus\n",
    "der Beobachtung gewonnenen Unsicherheiten durchgeführt wird, die in \n",
    "einer wiederholten Anpassung durch die aus der dann näherungsweise bekannten \n",
    "Modellerwartung gewonnenen Werte ersetzt werden.\n",
    "Bei den empfohlenen Werkzeugen *kafe2* und *phyFit* sind solche Umstände\n",
    "allerdings nicht notwendig, da sie das korrekte, oben skizzierte \n",
    "Likelihood-Verfahren für die Anpassung an Histogrammdaten nutzen. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: Anpassung an Histogramm-Daten\n",
    "\n",
    "Die Anpassung eines Modells an histogrammierte Daten wird durch die Funktion\n",
    "*hFit()* aus dem Paket *PhyPraKit.phyFit* unterstützt. Die Vorgehensweise ist ganz\n",
    "analog zur Anpassung von ungebinnten Daten. Von Anwenderseite müssen lediglich\n",
    "die Daten und die Verteilungsdichte bereit gestellt sowie Angaben zu den \n",
    "Ausgabeoptionen gemacht werden. Die passenden Kostenfunktionen, also das zweifache\n",
    "des negativen natürlichen Logarithmus der Poissonverteilung - oder, oft in \n",
    "ausreichender Näherung der Gaußverteilung - sind in *hFit* implementiert. \n",
    "\n",
    "Hier das vollständige Progammbeispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhyPraKit.phyFit import hFit\n",
    "\n",
    "#  the model function to fit\n",
    "def SplusB_model(x, mu = 6.987, sigma = 0.5, s = 0.2):\n",
    "  '''pdf of a Gaussian signal on top of flat background\n",
    "  '''\n",
    "  normal = np.exp(-0.5*((x-mu)/sigma)**2)/np.sqrt(2.*np.pi*sigma**2)\n",
    "  linear = 1. / (xmx-xmn) \n",
    "  return s * normal + (1-s) * linear \n",
    "\n",
    "nbins=40\n",
    "xmn = 1\n",
    "xmx = 10\n",
    "bedges=np.linspace(xmn, xmx, nbins+1)\n",
    "bcontents = np.array([1, 1, 1, 2, 2, 2, 4, 1, 0, 3, 1, 1, 0,\n",
    "                        2, 3, 3, 1, 1, 0, 2, 3, 2, 3, 1, 1, 8,\n",
    "                        6, 7, 9, 1, 0, 1, 2, 1, 3, 2, 1, 3, 2, 4])\n",
    "#  \n",
    "# ---  perform fit  \n",
    "#\n",
    "results = hFit(SplusB_model,\n",
    "    bcontents, bedges,      # bin entries and bin edges\n",
    "    limits=[('s', 0., None), ('sigma', 0., None)], # parameter limits\n",
    "    plot=True,              # plot data and model\n",
    "    plot_band=True,         # plot model confidence-band\n",
    "    plot_cor=True,          # plot profiles likelihood and contours\n",
    "    axis_labels=['x', 'y   \\  f(x, *par)'], \n",
    "    data_legend = 'peudo-data',    \n",
    "    model_legend = 'signal + background model' )\n",
    "\n",
    "# Print results \n",
    "# pnams, pvals, perrs, cor, chi2 = results\n",
    "print('\\n*==* histogram fit Result:')\n",
    "print(\" parameter names:       \", results[0])\n",
    "print(\" goodness-of-fit: {:.3g}\".format(results[4]))\n",
    "print(\" parameter values:      \", results[1])\n",
    "print(\" neg. parameter errors: \", results[2][:,0])\n",
    "print(\" pos. parameter errors: \", results[2][:,1])\n",
    "print(\" correlations : \\n\", results[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die grafische Ausgabe zeigt die angepasste Verteilungsdichte sowie deren Unsicherheit, die durch \n",
    "Propagation der Paramerunsicherheiten auf die Modellvorhersage bestimmt wird. \n",
    "Die rechtecktigen Flächen repräsentieren die Zahl der in den Daten beobachteten Einträge und \n",
    "entsprechen der klassischen Histogramm-Darstellung. Die um die Modellfunktion eingezeichneten\n",
    "Fehlerbalken zeigen den 68.3%-Konfidenzbereich für die unter Zugrundelegung der Poission-Verteilung\n",
    "erwartete Zahl an Einträgen in jedem Bin. Die Größe \"g.o.f\" (= \"goodness of fit\") ist die \n",
    "Differenz des beobachteten Werts von $n2l{\\cal L}$ und dem bestmöglichen Wert dieser Größe, \n",
    "den man erhält, wenn alle Daten auf der Modellvorhersage liegen, dem sog. \"Saturated Model\"; \n",
    "g.o.f. konvergiert für große Datensätze gegen die aus der Anpassung mit der Methode der kleinsten\n",
    "Fehlerquadrate bekannte Größe $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Vergleich mit der Gauß-Näherung ist durch Angabe einer weiteren Option\n",
    "`use_GaussApprox=True` beim Aufruf von *hFit* leicht möglich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "# ---  perform 2nd Fit with Gaussian cost function  \n",
    "#\n",
    "results = hFit(SplusB_model,\n",
    "    bcontents, bedges,      # bin entries and bin edges\n",
    "    limits=[('s', 0., None), ('sigma', 0., None)], # parameter limits\n",
    "    use_GaussApprox=True,   # use Gaussian approxmiation of Poisson distr.\n",
    "    plot=True,              # plot data and model\n",
    "    plot_band=True,         # plot model confidence-band\n",
    "    plot_cor=True,          # plot profiles likelihood and contours\n",
    "    axis_labels=['x', 'y   \\  f(x, *par)'], \n",
    "    data_legend = 'peudo-data',    \n",
    "    model_legend = 's+b, Gauss Approx.' )\n",
    "\n",
    "# Print results \n",
    "# pnams, pvals, perrs, cor, chi2 = results\n",
    "print('\\n*==* histogram fit Result:')\n",
    "print(\" parameter names:       \", results[0])\n",
    "print(\" goodness-of-fit: {:.3g}\".format(results[4]))\n",
    "print(\" parameter values:      \", results[1])\n",
    "print(\" neg. parameter errors: \", results[2][:,0])\n",
    "print(\" pos. parameter errors: \", results[2][:,1])\n",
    "print(\" correlations : \\n\", results[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-Likelihood und Methode der kleinsten Fehlerquadrate\n",
    "\n",
    "Die Methode der kleinsten Fehlerquadrate zur Anpassung von Modellen ist\n",
    "ein Spezialfall des maximum-Likelihood Verfahrens.\n",
    "\n",
    "Zunächst leiten wir ein auf der log-Likelihood Methode basierendes\n",
    "Anpassungsverfahren für Modellfunktionen an Messdaten her.\n",
    "Wir bezeichnen die zufällige Abweichung eines Messwertes vom wahren\n",
    "Wert mit dem Buchstaben $z$, die durch eine Verteilungsdichte $f_z$\n",
    "beschrieben wird. Im Falle von Messunsicherheiten ist $f_z$ häufig\n",
    "die Normalverteilung mit Erwartungswert Null und Standardabweichung $\\sigma$,\n",
    "${\\cal N}(z; \\sigma)$. Im mehrdimensionalen Fall für $n_d$ nicht\n",
    "notwendigerweise unabhängige Datenpunkte ist die multivariate\n",
    "Gaußverteilung ${\\cal N}(\\vec z; V)$ mit der Kovarianzmatrix $V$ relevant,\n",
    "\n",
    "\\begin{equation}\\label{equ-mStandardGauss}\n",
    "  {\\cal{N}}\\left(\\vec{z}, V) \\right) = \n",
    "\\frac{1} {\\sqrt{(2\\pi)^{n_d} \\det(V)} }\n",
    "\\cdot\n",
    "\\exp \\left( -\\frac{1}{2} \\vec{z}^T V^{-1} \\vec{z}\n",
    "\\right) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Die Zufallsgröße $z$ entspricht den Fluktuationen um den wahren Wert. \n",
    "Wenn es für den wahren Wert eine exakte theoretische Erwartung in Form\n",
    "eines parameterbehafteten Modells $f_i(\\vec p)$ mit einem Satz\n",
    "an Parametern $\\vec p$ gibt, lässt sich ein Messwert $y_i$ schreiben als\n",
    "\\begin{equation}\\label{equ-zplusw}\n",
    "y_i =  f_i(\\vec p) + z_i \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Üblicherweise führt man zum Test von parameterbehafteten, durch\n",
    "Funktionen beschriebenen Modellen mehrere Messungen an verschiedenen\n",
    "Stützstellen $x_i$ durch, man betrachtet also eine Modellfunktion\n",
    "$f(\\vec x; \\vec p)$.\n",
    "Die Verteilungsdichte, die alle Messungen beschreibt, sieht dann so aus:\n",
    "\n",
    "\\begin{equation}\\label{equ-mGauss}\n",
    "  {\\cal{N}}\\left(\\vec{y}, V, f(\\vec{x}, \\vec{p}) \\right) = \n",
    "  \\frac{1} {\\sqrt{(2\\pi)^{n_d} \\det(V)} } \\cdot\n",
    "  \\exp\\left( -\\frac{1}{2} (\\vec{y}-\\vec{f})^T V^{-1} (\\vec{y}-\\vec{f}) \\right) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Nach dem Maximum-Likelihood-Prinzip ist der beste Parametersatz durch\n",
    "den Punkt $\\hat{\\vec{ p}}$ im Parameterraum gegeben, für den die Likelihood\n",
    "maximal wird. Wie schon oben verwendet man das Zweifache des negativen natürlichen\n",
    "Logarithmus der Likelihood, $n2l{\\cal L}_{Gauss}$, der dann minimiert wird.\n",
    "\\begin{equation}\\label{equ-nlLGauss}\n",
    "-2\\, \\ln{\\cal{L_{\\rm Gauß}}}\\left( \\vec y, V, \\vec{f}(\\vec x, \\vec {p}) \\right)\n",
    "\\,=\\, \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)^T V^{-1}\n",
    "  \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)\n",
    "  + \\ln(\\det(V)) + n_d\\,\\ln(2\\pi) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Für die Bestimmung des Minimums von $n2l{\\cal L}_{Gauss}$ sind bzgl. der\n",
    "Parameter konstante Terme nicht relevant, man kann sie daher weglassen.\n",
    "Wenn die Normierung der Gaußverteilung, also der Term $\\det(V)$, nicht\n",
    "von den Parametern abhängt, vereinfacht sich der obige Ausdruck\n",
    "zum altbekannten Ausdruck für die quadratische Residuensumme mit \n",
    "Kovarianzmatrix, $S$:\n",
    "\n",
    "\\begin{equation}\\label{equ-chi2}\n",
    "S\\left(\\vec y, V, \\vec{f}(\\vec x, \\vec {p})\\right) \\,=\\,\n",
    "  \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)^T V^{-1}\n",
    "  \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)\n",
    "  + \\ln(\\det(V)) + d\\,\\ln(2\\pi) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Unter speziellen Bedingungen sind also die Minimierung\n",
    "von $n2l{\\cal{L}}$ und der quadratischen Summe der Residuen,\n",
    "$S$, äquivalent. Dieser einfache Fall ist aber nicht mehr\n",
    "gegeben, wenn relative, auf den Modellwert bezogene Unsicherheiten\n",
    "auftreten, oder wenn Unsicherheiten in Abszissen-Richtung\n",
    "behandelt werden sollen, die mit Hilfe einer Taylor-Entwicklung\n",
    "erster Ordnung von der Abszisse auf die Ordinate übertragen\n",
    "und damit von der Ableitung des Modells nach $x$ und so\n",
    "auch von den Parameterwerten abhängen.\n",
    "Die praktische Regel lautet daher, möglichst immer die Likelihood\n",
    "zu verwenden, und nur in gut begründeten, berechtigten Fällen\n",
    "auf die Methode der kleinsten Fehlerquadrate zurück zu greifen.\n",
    "Allerdings ist man bei der Wahl der verfügbaren numerischen\n",
    "Werkzeuge stark eingeschränkt, wenn man diese Empfehlung\n",
    "umsetzen möchte.\n",
    "\n",
    "Sollen Problemstellungen mit nicht Gauß-förmigen Verteilungen\n",
    "behandelt werden, ist die Verwendung von log-Likelihoodverfahren\n",
    "unumgänglich. Das Aufstellen entsprechender Likelihood-Funktionen\n",
    "zur Behandlung spezieller Problemstellungen gehört in der\n",
    "wissenschaftlichen Praxis heute zum Standard. Dank sehr leistungsfähiger\n",
    "Algorithmen zur numerischen Minimierung in hoch-dimensionalen\n",
    "Parameterräumen und auch Dank moderner Computertechnik stellt\n",
    "die Verwendung korrekter Likelihood-Verfahren kein\n",
    "unüberwindliches Problem mehr dar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratkische Hinweise\n",
    "\n",
    "In der Praxis sind die oben beschriebenen Verfahren nur in Kombination mit \n",
    "numerischen Verfahren zur Minimierung von skalaren Funktionen in mehr- oder\n",
    "sogar hoch-dimensionalen Räumen und Programmcode zur Verwaltung der Daten\n",
    "und ihrer Unsicherheiten durchführbar. Obwohl in seltenen Fällen auch analytische\n",
    "Lösungen existieren, nutzt man in der Praxis fast ausschließlich Programmpakete\n",
    "zur Durchführung von Anpassungen; analytische (Teil-)Lösungen müssen nur\n",
    "eingesetzt werden, wenn es um zeitkritische Problemstellungen geht. \n",
    "\n",
    "## Konstruktion der Kovarianzmatrix\n",
    "\n",
    "Die Kovarianzmatrix der Daten bildet die Unsicherheiten Gauß-verteilter \n",
    "Eingabedaten vollständig ab. Wird sie bei der Anpassung berücksichtigt, \n",
    "werden alle zwischen den Daten unabhängigen und korrelierten Unsicherheiten\n",
    "beim Anpassungprozess in das Endergebnis propagiert - eine klassische \n",
    "Fehlerrechnung von Hand ist dann nicht mehr notwendig. \n",
    "\n",
    "Fassen wir zunächst kurz die wesentlichen Eigenschaften der\n",
    "Kovarianzmatrix ${\\bf V}$ zusammen:\n",
    "\n",
    "- ${\\bf V} = \\left(\\mathrm{V}_{ij}\\right)$ ist eine symmetrische Matrix;\n",
    "- sie hat die Dimension $n_d$, die der Anzahl der Messwerte entspricht;\n",
    "- für unabhängige Messwerte ist die Matrix diagonal;\n",
    "- die Nebendiagonalelemente $V_{ij},\\,{\\small i\\ne j}$ lassen sich verstehen als\n",
    "  das Produkt der gemeinsamen Unsicherheiten $\\sigma^g_i$ und $\\sigma^g_j$ der \n",
    "   Messungen $i$ und $j$;\n",
    "- die Kovarianzmatrix-Elemente für voneinander unabhängige Unsicherheiten\n",
    "  werden addiert.\n",
    "\n",
    "Gerade der letzte Punkt ist von entscheidender Bedeutung, denn er erlaubt es,\n",
    "die vollständige Kovarianzmatrix sukzessive aus einzelnen Beiträgen zur\n",
    "Unsicherheit aufzubauen, mit der Konstruktionsvorschrift:\n",
    "\n",
    "- Unsicherheiten der Messwerte werden nach verschiedenen, unabhängigen\n",
    "  Quellen aufgeschlüsselt.\n",
    "- Unabhängige Unsicherheiten jeder einzelnen Messung werden quadratisch in\n",
    "  den Diagonalelementen aufaddiert.\n",
    "- Allen Messwerten oder Gruppen von Messungen gemeinsame absolute oder\n",
    "  relative Unsicherheiten werden quadratisch in den betreffenden\n",
    "  Diagonal- und Nebendiagonalelementen $V_{ii}$, $V_{jj}$ und $V_{ij}$ aufaddiert.\n",
    "\n",
    "Unsicherheiten in Abszissenrichtung, gegeben durch eine Kovarianzmatrix\n",
    "${\\bf V}^x$, können berücksichtigt werden, in dem man mit sie Hilfe der\n",
    "Modellvorhersage $f(\\vec x, \\vec p)$ per Taylor-Entwicklung in erster Ordnung\n",
    "in y-Richtung transformiert und dann zur Kovarianz-Matrix der Datenpunkte in\n",
    "y-Richtung, ${\\bf V}^y$, addiert:\n",
    "\n",
    "\\begin{equation}\\label{equ-xyCovariance}\n",
    "  V_{ij} = (V^y)_{ij} \n",
    "        +  \\frac{\\partial f}{\\partial x_i} \\frac{\\partial f}{\\partial x_j} (V^x)_{ij} \n",
    "\\end{equation}\n",
    "\n",
    "Insgesamt ergeben sich so acht Arten von Unsicherheiten, nämlich \n",
    "unabhängige und / oder korrelierte\n",
    "absolute und / oder relative Unsicherheiten in\n",
    "$x$- und / oder $y$-Richtung.\n",
    "\n",
    "Zum Bau der Kovarianzmatrix setzt man idealerweise Programmcode ein. Die\n",
    "wenigsten gängigen Anpassungsprogramme bringen dafür direkt Optionen mit,\n",
    "sondern es muss die vollständige Kovarianzmatrix als Parameter übergeben\n",
    "werden.\n",
    "Das Paket {\\em kafe2} enthält die Methode   \n",
    "`add_error(err_val, axis=?, correlation=?, relative=?, reference=?)`   \n",
    "mit deren Hilfe einzelne Komponenten der Unsicherheit hinzugefügt werden\n",
    "können. Die Interfaces zu verschiedenen Anpassungsprogrammen\n",
    "im Paket *PhyPraKit* sehen ebenfalls die Angabe einzelner Komponenten\n",
    "der Unsicherheit als Parameter vor, wenn die entsprechenden Anpassungspakete\n",
    "dies unterstützen.\n",
    "Alle acht Arten von Unsicherheiten können direkt nur mit *kafe2*\n",
    "*PhyPraKit.phyFit* behandelt werden. \n",
    "\n",
    "## Berücksichtigung externer und eingeschränkter Parameter\n",
    "\n",
    "Häufig hängen Modellfunktionen von externen, mit Unsicherheiten behafteten \n",
    "Parametern ab, die z.$\\,$B. in einer Hilfsmessung bestimmt wurden oder aus der\n",
    "Literatur stammen. Dies können die Ergebnisse von Kalibrationsmessungen sein,\n",
    "oder auch Natur- oder Apparatekonstanten. Statt die Effekte der Paramterunsicherheiten \n",
    "mit Hilfe einer händischen Fehlerrechnung auf das Endergebnis zu propagieren, \n",
    "können  sie auch als eingeschränkte Parameter (\"constrained parameter\") direkt \n",
    "in der Anpassung berücksichtigt werden. \n",
    "Dazu werden solche Parameter gleichzeitig als freie Parameter in der \n",
    "Anpassung und als Messgrößen eingeführt - d.$\\,$h. ein entsprechender Term \n",
    "zur log-Likelihood hinzugefügt. \n",
    "\n",
    "Notwendig ist bisweilen auch eine Methode, mit deren Hilfe Parameter auf \n",
    "feste Werte fixiert werden können.\n",
    "Der Einfluss externer Parameter kann damit auch untersucht werden, indem\n",
    "man sie nacheinander auf ihren Erwartungswert und die jeweiligen oberen bzw. \n",
    "untere Grenzen ihres Konfidenzbereichs fixiert und die Veränderungen des \n",
    "Anpassungsergebnisses beobachtet.\n",
    "\n",
    "Das temporäre Fixieren und wieder frei geben von Parametern kann auch hilfreich \n",
    "sein, wenn eine komplexe Anpassung nicht zum globalen Minimum konvergiert. Man \n",
    "kann dann einen oder einige Parameter in der Nähe des erwarteten Wertes fixieren \n",
    "und eine Minimierung bezüglich der übrigen Parameter vornehmen. Wenn man dann an \n",
    "diesem temporären Minimum die fixierten Parameter wieder frei gibt, sollte \n",
    "die Anpassung zum globalen Minimum konvergieren. \n",
    "\n",
    "Ein Problem während des Anpassungsprozesses stellen oft auch Parameter dar,\n",
    "die temporär Werte in mathematisch nicht definierten Wertebereichen oder in \n",
    "unphysikalischen Bereichen annehmen (negative Werte unter Wurzeln, negative\n",
    "Massen etc). Das Setzen von Limits zum Ausschluss solcher Parameterbereiche\n",
    "ist eine wichtige Option für jedes Anpassungspaket. \n",
    "\n",
    "Bei nichtlinearen Problemstellungen gibt es häufig neben dem globalen Minimum\n",
    "weitere Nebenminima - oder sogar mehrere oder viele gleichwertige Lösungen. \n",
    "In solchen Fällen werden Startwerte für die Parameter in der Nähe\n",
    "einer \"vernünftigen\" Lösung benötigt. Auch diese Möglichkeit muss ein \n",
    "Anpassungspaket bieten. Da im Laufe des besseren Verständnisse aus manchem \n",
    "einfachen Problem später ein nichtlineares Problem werden kann, ist es eine \n",
    "gute Angewohnheit, grundsätzlich immer Startwerte zu setzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispiel mit allen acht oben genannten Arten von Unsicherheiten ist in der Code-Zelle\n",
    "unten gezeigt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel zur Anpassung an x/y-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from PhyPraKit.phyFit import mnFit, xyFit\n",
    "\n",
    "#\n",
    "# *** Example of an application of phyFit.mFit()\n",
    "#\n",
    "# define the model function to fit\n",
    "def exp_model(x, A=1., x0=1.):\n",
    "  return A*np.exp(-x/x0)\n",
    "\n",
    "# another model function\n",
    "def poly2_model(x, a=0.1, b=1., c=1.):\n",
    "  return a*x**2 + b*x + c\n",
    "\n",
    "# set model to use in fit\n",
    "#fitmodel=exp_model  # also try poly2_model !\n",
    "fitmodel=poly2_model  # also try exp_model!\n",
    "  \n",
    "# the data ...\n",
    "data_x = [0.0, 0.2, 0.4, 0.6, 0.8, 1., 1.2,\n",
    "          1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6]\n",
    "data_y = [1.149, 0.712, 0.803, 0.464, 0.398, 0.354, 0.148,\n",
    "          0.328, 0.181, 0.140, 0.065, 0.005,-0.005, 0.116]\n",
    "# ... and components of the uncertaity \n",
    "sabsy = 0.07 # independent y\n",
    "srely = 0.05 # 5% of model value\n",
    "cabsy = 0.04 # correlated\n",
    "crely = 0.03 # 3% of model value correlated\n",
    "sabsx = 0.05 # independent x\n",
    "srelx = 0.04 # 4% of x\n",
    "cabsx = 0.03 # correlated x\n",
    "crelx = 0.02 # 2% of x correlated\n",
    "\n",
    "# perform fit to data with function xyFit using class mnFit\n",
    "results = xyFit(fitmodel,      # the model function\n",
    "           data_x, data_y,     # x and y data\n",
    "           sx=sabsx,           # the error components defined above\n",
    "           sy=sabsy,\n",
    "           srelx=srelx,\n",
    "           srely=srely,\n",
    "           xabscor=cabsx,\n",
    "           xrelcor=crelx,\n",
    "           yabscor=cabsy,\n",
    "           yrelcor=crely,\n",
    "#           p0=(1., 0.5),     # start values of parameters (taken from model if not given)\n",
    "#           constraints=['A', 1., 0.03],    # constraint(s)\n",
    "#           constraints=[0, 1., 0.03]       # alternative specificaiton of constraints\n",
    "#           limits=('A', 0., None),  # parameter limits\n",
    "#           fixPars = ['A'],         # parameter(s) to be fixed to start value \n",
    "           use_negLogL=True,         # use full n2lL (default)\n",
    "           plot=True,                # plot data and model \n",
    "           plot_band=True,           # plot model uncerctainty\n",
    "           plot_cor=True,            # plot profiels and contours\n",
    "           quiet=True,               # silent output it True\n",
    "           axis_labels=['x', 'y   \\  f(x, *par)'], \n",
    "           data_legend = 'pseudo data',    \n",
    "           model_legend = 'model')\n",
    "\n",
    "# Print results \n",
    "print('\\n*==* xyFit Result:')\n",
    "pnams, pvals, perrs, cor, chi2 = results\n",
    "print(\" parameter names:       \", pnams)\n",
    "print(\" goodness-of-fit: {:.3g}\".format(chi2))\n",
    "print(\" parameter values:      \", pvals)\n",
    "print(\" neg. parameter errors: \", perrs[:,0])\n",
    "print(\" pos. parameter errors: \", perrs[:,1])\n",
    "print(\" correlations : \\n\", cor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
