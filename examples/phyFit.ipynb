{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Jupyter-Notebook-Tutorial:   \n",
    "\n",
    "#    Anpassung von Modellen an Messdaten mit der Maximum-Likelihood-Methode\n",
    "\n",
    "                                                    Günter Quast, Juni 2021\n",
    "***\n",
    "## Zusammenfassung\n",
    "\n",
    "Dieses Tutorial führt in die Grundlage der Anwendung der Maximum-Likelihood-Methode\n",
    "zur Anpassung an Messdaten ein. Für viele in der Praxis häufig auftretende Probemstellungen\n",
    "ist die in vielen Anwendungen und Programmpaketen implementierte Methode der \"kleinsten\n",
    "Fehlerquadrate\" (\"least squares\", LSQ) nicht adäquat. Dann die auf soliden mathematiscen\n",
    "Grundlagen beruhende Maximum-Likelihood-Methode (\"Maximum Likelihood Estimation\", MSE)\n",
    "zur Schätung der Modellparameter und ihrer Vertrauensbereiche angewandt werden. \n",
    "\n",
    "Dieses Tutorial in Form eines Jupyter-Notebooks bietet neben kurzen Erläuterungen der \n",
    "mathematischen Grundlagen insbesondere viele Code-Beispiele in der Sprache *Python*, \n",
    "die die Maximum-Likelihood-Methode an typischen Beispielen erläutern und zur Anwendung\n",
    "in eigenen Anpassungsproblemen angepasst werden können.\n",
    "\n",
    "Der Beispielcode nutzt das *Python*-Paket [iminuit](https://iminuit.readthedocs.io/en/stable/)\n",
    "zur numerischen Optimierung und Analyse der Unsicherheiten sowie die im wissenschaftlichen\n",
    "Rechnen mit *Python* weit verbreiteten Bibliotheken *numpy*, *scipi* und *matplotlib*.\n",
    "Als anwendungsorientierte Zwischenebene wird das Paket \n",
    "[PhyPraKit.phyFit](https://phyprakit.readthedocs.io/en/latest/) verwendet.\n",
    "\n",
    "\n",
    "\n",
    "## Inhaltsübersicht:\n",
    "\n",
    "1. Anpassung von Modellen an Messdaten\n",
    "\n",
    "2. Mathematische Grundlagen der Parameterschätzung\n",
    "\n",
    "   2.1 Bestimmung der Parameterunsicherheiten und Intervallschätzung\n",
    "    \n",
    "     - Parameterschätzug mit dem Maximum-Likelihood Verfahren\n",
    "\n",
    "     - Bestimmung der Parameterunsicherheiten und Intervallschätzung\n",
    "\n",
    "     - Alternative Methode zur Bestimmung der Parameterunsicherheiten: Profil-Likelihood\n",
    "\n",
    "     - log-Likelihood Anpassung einer Exponentialfuntion\n",
    "\n",
    "3. Anpassung von Verteilungsdichten an Histogramme\n",
    "\n",
    "   3.1 Beispiel \n",
    "\n",
    "4. Maximum-Likelihood und Methode der kleinsten Fehlerquadrate\n",
    "\n",
    "   4.1 Konstruktion der Kovarianzmatrix\n",
    "\n",
    "   4.2 Beispiel zur Anpassung an x/y-Daten mit komplexen Unsicherheiten  \n",
    "\n",
    "5. Beurteilung der Qualität von Anpassungsverfahren\n",
    "\n",
    "   5.1 Überprüfung mittels Monte-Carlo - Methode (\"toy-MC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Grundsätzliches zu Jupyter Notebooks\n",
    "\n",
    "Diese Datei vom Typ `.ipynb` enthält ein Tutorial als `Jupyter notebook`.\n",
    "*Jupyter* bietet eine Browser-Schnittstelle mit einer (einfachen) Entwicklungsumgebung\n",
    "für *Python*-Code und erklärende Texte im intuitiven *Markdown*-Format.\n",
    "Die Eingabe von Formeln im *LaTeX*-Format wird ebenfalls unterstützt.\n",
    "\n",
    "Eine Zusammenstellung der wichtigsten Befehle zur Verwendung von *Jupyter* als Arbeitsumgebung\n",
    "findet sich im Notebook\n",
    "[*JupyterCheatsheet.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/JupyterCheatsheet.ipynb).\n",
    "Grundlagen zur statistischen Datenauswertung finden sich in den Notebooks \n",
    "[*IntroStatistik.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/IntroStatistik.ipynb)\n",
    "und\n",
    "[*Fehlerrechnung.ipynb*](https://git.scc.kit.edu/yh5078/datenanalyse/-/blob/master/jupyter/Fehlerrechnung.ipynb).\n",
    "\n",
    "In *Jupyter* werden Code und Text in jeweils einzelne Zellen eingegeben. \n",
    "Aktive Zellen werden durch einen blauen Balken am Rand angezeigt.\n",
    "Zellen können sich in zwei Zuständen befinden: im Edit-Mode ist das Eingabefeld weiß, \n",
    "im Command-Mode ist es ausgegraut.\n",
    "Durch Klicken in den Randbereich wird der Command-Mode gewählt, ein Klick in das Textfeld einer\n",
    "Code-Zelle schaltet in den Edit-Mode.\n",
    "Die Taste `esc` kann ebenfalls verwendet werden, um den Edit-Mode zu verlassen.\n",
    "\n",
    "Die Eingabe von `a` im Command-Mode erzeugt eine neue leere Zelle oberhalb der aktiven Zelle, \n",
    "`b` eine unterhalb. Eingabe von `dd` löscht die betreffende Zelle.\n",
    "\n",
    "Zellen können entweder den Typ `Markdown` oder `Code` haben.\n",
    "Die Eingabe von `m` im Command-Mode setzt den Typ Markdown, Eingabe von `y` wählt den Typ Code.\n",
    "\n",
    "Prozessiert - also Text gesetzt oder Code ausgeführt - wird der Zelleninhalt durch Eingabe von\n",
    "`shift+return`, oder auch `alt+return` wenn zusätzlich eine neue, leere Zelle erzeugt werden soll.\n",
    "\n",
    "Die hier genannten Einstellungen sowie das Einfügen, Löschen oder Ausführen von Zellen sind\n",
    "auch über das PullDown-Menü am oberen Rand verfügbar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Likelihood-Anpassung von Modellen an Messdaten\n",
    "***\n",
    "Die Physik als \"Theorie-geleitete Erfahrungswissenschaft\" lebt vom\n",
    "Zusammenspiel theoretischer Hypothesenbildung und deren stringenter\n",
    "Überprüfung im Experiment. Bei der Interpretation von Messdaten kommt\n",
    "es daher zunächst darauf an, ob sie überhaupt einem erwarteten Modell \n",
    "entsprechen. Erst, wenn diese Frage mit \"ja\" beantwortet werden kann, \n",
    "ist die Extraktion vom Modellparametern sinnvoll.\n",
    "\n",
    "Leider tragen die meisten der häufig verwendeten Werkzeuge in der \n",
    "Voreinstellung dieser Anforderung nicht Rechnung, sondern zielen \n",
    "auf eine möglichst genaue Parametrisierung der Daten ab. \n",
    "Bei Vorgabe von unpassenden Modellen werden dazu die Parameterfehler\n",
    "so weit vergrößert, dass die Parametrisierung möglichst genau passt.\n",
    "\n",
    "Um den besonderen Bedingungen der Datenauswertung in der Physik\n",
    "gerecht zu werden, ist die Verwendung von eigenem Programmcode, \n",
    "idealerweise in der Programmiersprache *Python* in einem \n",
    "Jupyter-Notebook, häufig notwendig, da die meisten mittels einer\n",
    "grafischen Oberfläche zu bedienenden Werkzeuge zur statistischen\n",
    "Datenauswertung den Anforderungen nicht oder nur zum Teil genügen. \n",
    "\n",
    "Wichtige Anforderugnen an eine Methode zur Modellanpassung sind:\n",
    "\n",
    "- Alle Messunsicherheiten sollten direkt bei der Modellanpassung berücksichtigt\n",
    "  und entsprechend modelliert werden, um sie vollständig und korrekt auf die\n",
    "  Unsicherheiten der Parameter zu propagieren. Eine später von Hand\n",
    "  durchgeführte Fehlerfortpflanzung ist aufwändig und funktioniert nur\n",
    "  in einfachsten Fällen.   \n",
    "\n",
    "- Häufig treten auch relative Unsicherheiten als Bruchteil des wahren \n",
    "  Messwerts auf, z.$\\,$B. bei Kalibrationsunsicherheiten o.\\,Ä..\n",
    "  Wenn solche relative Unsicherheiten korrekt behandelt werden müssen, \n",
    "  gelingt das mit der in vielen Anpassungsprogrammen implementierten\n",
    "  Methode der kleinsten Fehlerquadrate nur noch eingeschränkt. \n",
    "  Bessere Parameterschätzungen nutzen die Maximum-Likelihood-Methode \n",
    "  (\"MLE\", Maximum Likelihood Estimation) mit an das Prolem angepassten\n",
    "  Likelihood-Funktionen. \n",
    "\n",
    "- In der Quanten-, Kern- und Teilchenphysik treten als Messgrößen\n",
    "  Zählraten in Abhängigkeit von Energie, Streuwinkel oder anderen\n",
    "  Größen auf, die üblicherweise als Häufigkeitsverteilung (Histogramm)\n",
    "  dargestellt werden. Die Einträge in einem Intervall eines Histogramms\n",
    "  (einem sog. `Bin`) sind aber nicht Gauß-, sondern Poisson-verteilt. \n",
    "  Auch näherungsweise können Bins mit Null Einträgen mit einer - oft immer\n",
    "  noch verwendeten - angepassten Methode der kleinsten Fehlerquadrate\n",
    "  nicht berücksichtigt werden. Zur Anpassung von Modellen an Histogramme\n",
    "  müssen also ebenfalls maximum-likelihood Methoden angewendet werden.\n",
    "  \n",
    "- Bei der Analyse von sehr kleinen Zählraten bei nur selten eintretenden\n",
    "  Ereignissen kann schon die Notwendigkeit zur Aufteilung in Bins zur einem\n",
    "  Informationsverlust und zu einer Verzerrung des Ergebnisses führen.\n",
    "  In solchen Fällen sollte ein ungebinnter Maximum-Likelihod-Fit zur\n",
    "  Anwendung kommen.\n",
    "\n",
    "- Bei der Anpassung an zweidimensonale Datenppunkte $(x_i, y_i)$ sollten \n",
    "  die verwendeten Werkzeuge Unsicherheiten in Abszissenrichtung (entlang\n",
    "  der \"x-Achse\") zusätzlich zu denen in Ordinatenrichtung behandeln können. \n",
    "  Die Unterstützung von korrelierten Unsicherheiten ist ebenfalls notwendig, u\n",
    "  m typische Charakteristika von Messgeräten, wie korrelierte \n",
    "  Kalibrationsunsicherheiten zusätzlich zu den unabhängigen \n",
    "  Digitalisierungsunsicherheiten und Rauschbeiträgen, darstellen zu können.\n",
    "\n",
    "Leider gibt es kaum Anpassungswerkzeuge, die alle genannten Anforderungen\n",
    "gleichzeitig erfüllen. Deshalb wurde am ETP in zahlreichen Bachelor-Arbeiten\n",
    "ein quelloffenes Anpassungswerkzeug, [kafe2](https://github.com/dsavoiu/kafe2)\n",
    "entwickelt, das die genannten Aspekte abdeckt. Allerdings ist der Programmcode\n",
    "wegen der großen Zahl an Optionen und Methoden sehr komplex und für Einsteiger\n",
    "unübersichtlich.\n",
    "\n",
    "Zur Illustration der Vorgehensweise wird daher in diesem Tutorial ein\n",
    "einziges, sehr flexibles Werkzeug zur numerischen Optimierung und \n",
    "Analyse der Parameterunsicherheiten verwendet, nämlich das am CERN \n",
    "entwickelte Paket `Minuit` bzw. dessen Python-Interface \n",
    "[iminuit](https://iminuit.readthedocs.io/en/stable/). \n",
    "Eine schlanke Implementierung von Anwendungsbeispielen bietet das Paket \n",
    "[PhyPraKit.phyFit](https://phyprakit.readthedocs.io/en/latest/), das \n",
    "hier verwendet wird, um die Grundlagen der Anpassung von Modellen an \n",
    "Messdaten mit der Maximum-Likelihood Methode darzustellen.  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematische Grundlagen der Parameterschätzung\n",
    "***\n",
    "\n",
    "In diesem Tutorial werden ausgehend vom Maximum-Likelihood-Prinzip \n",
    "zunächst typische Anwendung zur Anpassung von Verteilungsdichten an\n",
    "Messdaten diskutiert und dann die Verbindung zur altbekannten Methode\n",
    "der kleinsten Quadrate zur Anpassung von parameterbehafteten \n",
    "Modellfunktionen $y_i = f(x_i; *par )$ an zweidimensionale Datenpunkte\n",
    "$(x_i, y_i)$ dargestellt. \n",
    "\n",
    "Ausgangspunkt der Überlegungen ist der auf der Maximum-Likeliood Methode\n",
    "basierende Formalismus zur Parameterschätzung von Verteilungsdichten, \n",
    "die den Messergebnissen zu Grunde liegen. Manchmal sind die Parameter der \n",
    "Verteilungsdichte selbst interessant, z.$\\,$B. bei der Lebensdauerverteilung \n",
    "von quantenmechanischen Zuständen oder von (Elementar-)Teilchen.\n",
    "Häufig treten Parameter von Verteilungsdichten aber auch nur als \n",
    "\"Störparameter\" auf, die statistische Fluktuationen um einen angenommenen\n",
    "wahren Wert beschreiben. Die Behandlung von Messunsicherheiten von \n",
    "Datenpunkten $(x_i, y_i)$ ist dafür ein Beispiel. \n",
    "\n",
    "\n",
    "### Parameterschätzug mit dem Maximum-Likelihood Verfahren  \n",
    "\n",
    "Beginnen wir an dieser Stelle mit der einfachsten Problemstellung, \n",
    "der Schätzung der Parameter der Verteilungsdichte einer Grundgesamtheit,\n",
    "aus der eine endliche Stichprobe (=\"Daten\") gezogen werden. \n",
    "\n",
    "Wir gehen von einer Verteilungsdichte ${\\rm pdf}(x, \\vec p)$ aus,\n",
    "die von einer Anzahl von Parametern $p_j$ abhängt. Ein Datensatz\n",
    "$\\vec x$ besteht aus ${n_d}$ voneinander unabhängigen\n",
    "Einzelmessungen $x_i$.\n",
    "\n",
    "Nach dem Maximum Likelihood-Prinzip ist der beste Schätzwert der\n",
    "Parameter ${\\hat{ \\vec {p}}}$ gegeben durch die Werte, die die\n",
    "sog. `Likelihood`, das Produkt der Werte der $pdf$ für die\n",
    "Einzelmessungen, ${\\rm pdf}(x_i, \\vec p)$, maximieren:\n",
    "\n",
    "\\begin{equation}\\label{equ:Likelihood1}\n",
    "  {\\cal L}(\\vec x, \\vec p) =\n",
    "  \\displaystyle\\prod_{i=1}^{n_d} \\, {\\rm pdf}(x_i, \\vec p) \\,.\n",
    "\\end{equation}  \n",
    "\n",
    "Um unhandlich kleine Werte des Produkts zu vermeiden, aus Gründen\n",
    "der numerischen Stabilität und auch aus mathematischen Gründen\n",
    "(s. \"Fisher-Information\") verwendet man den negativen Logarithmus\n",
    "von $\\cal L$, die \"negative log-Likelihood\" $nl\\cal L$, gegeben durch\n",
    "\\begin{equation}\\label{equ:Likelihood2}\n",
    "  {nl\\cal L}(\\vec x, \\vec p) =\n",
    "  -\\displaystyle\\sum_{i=1}^{n_d} \\, \\ln\\left({\\rm pdf}(x_i, \\vec p)\\right) \\,.\n",
    "\\end{equation}  \n",
    "\n",
    "Die negative log-Likelihood-Funktion wird als Funktion der Parameter\n",
    "aufgefasst und bezüglich der Parameter minimiert. Üblicherweise verwendet\n",
    "man sie als Kostenfunktion in numerischen Optimierungsverfahren.\n",
    "\n",
    "In der Physik ist es auch üblich, einen mit Zwei multiplizierten Wert zu verwenden, \n",
    "${n2l\\cal L}(\\vec x, \\vec p) = 2 \\cdot {nl\\cal L}(\\vec x, \\vec p)$.\n",
    "Wie wir unten sehen werden, entspricht dies in Spezialfällen dem Wert der\n",
    "Residuensumme, $S$, der nach dem von Gauß vorgeschlagenen Verfahren der \n",
    "kleinsten Fehlerquadrate häufig als Kostenfunktion verwendet wird. \n",
    "\n",
    "Es lässt sich zeigen, dass das log-Likelihood-Verfahren unter allen \n",
    "Verfahren zur Parameterschätzung optimal ist, also die kleinste Varianz \n",
    "der Parameterwerte liefert. Auch dies ist ein Grund, dem Likelihood-Verfahren\n",
    "den Vorzug zu geben, wenn man maximalen Informationsgewinn aus bisweilen\n",
    "extrem aufwändigen und damit teuren Experimenten ziehen möchte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für ein digitales Tutorial ist es an dieser Stelle Zeit für ein konkretes Beispiel.\n",
    "Wir betrachten Daten, an die eine Gauß-Verteilung angepasst wird. \n",
    "\n",
    "Mit Hilfe des folgen Codefragments werden zunächst (Pseudo-)Daten erzeugt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate Gaussian-distributed data\n",
    "mu0=2.\n",
    "sig0=0.5\n",
    "np.random.seed(314159)  # initialize random generator\n",
    "data = mu0 + sig0 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gaussverteilung mit Erwartungswert $\\mu$ und Standardabweichung $\\sigma$ ist  \n",
    "\n",
    "$Gauss(x; \\mu, \\sigma) \\,=\\, \\frac{1}{\\sqrt{2\\pi} \\, \\sigma} \\exp -{\\frac{(x-\\mu)^2}{2 \\sigma^2}}$\n",
    "\n",
    "Das Zweifache des negativen natürlichen Logarithmus der Likelihood davon ist\n",
    "\n",
    "$-2\\, \\ln(Gauss(x; \\mu, \\sigma))\\,=\\, \\left(\\frac{x-\\mu}{\\sigma}\\right)^2 + 2\\ln(\\sigma) + \\ln (2\\pi) $\n",
    "\n",
    "Den von allen Variablen unabhängigen Term $\\ln(2\\pi)$ kann man weglassen, \n",
    "weil er auf die Lage des Minimums keinen Einfluss hat. \n",
    "\n",
    "Die Kostenfunktion $n2l{\\cal L}$ ist die Summe solcher Ausdrücke über alle $n_d$ Werte\n",
    "einer Messreihe:\n",
    "\n",
    "$n2l{\\cal L} = 2 n_d \\ln(\\sigma) + \n",
    "                \\displaystyle \\sum_i^{n_d} \\left(\\frac{x_i-\\mu}{\\sigma}\\right)^2$\n",
    "\n",
    "Der Code dazusieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function: 2 * negative log likelihood of Gauß;\n",
    "def myCost(mu=1., sigma=1.):\n",
    "  # simple -2*log-likelihood of a 1-d Gauss distribution\n",
    "  r = (data-mu)/sigma\n",
    "  return np.sum( r*r) + 2.*len(data)*np.log(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obwohl in diesem Fall eine analytische Minimierung der Kostenfunktion möglich wäre, wollen wir ein numerisches Verfahren nutzen. Verwendet wird im Beispiel in der Code-Zelle unten das Paket MINUIT mit Hilfe des *Python*-basiertes Interfaces *iminuit*. \n",
    "Neben der Minimierung einer skalaren, von Parametern abhängigen \"Kostenfunktion\" bietet *iminuit* auch umfangreiche Methoden zur Steuerung der Anpassung und zur Analyse der Parameterunsicherheiten. \n",
    "\n",
    "Für unser einfaches Problem sieht der *iminuit*-Code so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for iminuit vers. >2.0\n",
    "from iminuit import Minuit\n",
    "\n",
    "# initialize Minuit object\n",
    "m = Minuit(myCost, mu=1., sigma=1.)\n",
    "m.errordef = 1.         # internal parameter, needed to control uncertainty analysis\n",
    "\n",
    "# perform optimzation\n",
    "m.migrad()\n",
    "\n",
    "# print results\n",
    "print(\"parameter names: \", m.parameters)\n",
    "print(\"best-fit values: \", m.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas einfacher wird die Anpassung mit Hilfe einer zusätzlichen Software-Ebene, \n",
    "die *Minuit* und das mächtige, aber damit auch notwendigerweise komplizierte Interface \n",
    "kapselt und so typische Anwendungsfälle vereinfacht, wie z.$\\,$B. das \n",
    "Werkzeug [*kafe2*](https://github.com/dsavoiu/kafe2), \n",
    "oder das schlanke, im Paket *PhyPraKit* enthaltene Paket *phyFit*. \n",
    "Für einfache Anpassungen mit *iminuit*\n",
    "ist die Interface-Funktion *mFit* geeignet. Übergeben wird lediglich die Kostenfunktion; \n",
    "Rückgabewerte sind die Namen und optimalen Parameterwerte, die in der Kostenfunktion als\n",
    "Keyword-Argumente angegeben sind. Zurückgegeben wird weiter eine Schätzung der \n",
    "Parameterunsicherheiten und eine Größe zur Einschätzung der Qualität der Anpassung. \n",
    "Über diese letztgenannten Größen wird weiter unten noch zu sprechen sein.  \n",
    "\n",
    "Das Anpassungsbeispiel mit *mFit* sieht wie folgt aus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhyPraKit.phyFit import mFit  \n",
    "\n",
    "fit_results= mFit(myCost)\n",
    "\n",
    "# Print results\n",
    "print('\\n*==* user-defined cost: Fit Result:')\n",
    "print(\" parameter names:       \", fit_results['parameter names'])\n",
    "print(\" parameter values:      \", fit_results['parameter values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe zeigt, dass die so bestimmten Parameterwerte den für die Erzeugung\n",
    "der Pseudo-Daten verwendeten Paramtern entspricht. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bestimmung der Parameterunsicherheiten und Intervallschätzung\n",
    "\n",
    "Neben den besten Werten aller Parameter sind ebenso deren Unsicherheiten\n",
    "bedeutsam. Die Varianz der Parameter, oder besser die Kovarianzmatrix\n",
    "der Unsicherheiten aller Parameter, lässt sich aus dem Verlauf der\n",
    "log-Likelihood am Minimum bestimmen. Anschaulich ist ein Parameter\n",
    "um so genauer bestimmt, je \"schärfer\" das Minimum ausgeprägt ist;\n",
    "dies kann über die zweiten Ableitungen der log-Likelihood nach den\n",
    "Parametern quantifiziert werden (Cramér-Rao-Fréchet Grenze): \n",
    "\n",
    "\\begin{equation}\\label{equ-Vij}\n",
    " \\left( {\\bf V} ^{-1}\\right)_{ij} \\ge{ \\left. \n",
    "\\frac {{\\partial}^2 \\ln{\\cal{L}}()}\n",
    "{\\partial p_i \\, \\partial p_j} \n",
    "\\right|}_{\\hat p_i \\hat p_j}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "$\\left( {\\bf V} ^{-1}\\right)_{ij}$ sind die Elemente der Inversen der \n",
    "Kovarianzmatrix ${\\bf V}$ der Parameterunsicherheiten und durch \n",
    "die Krümmungen der log-Likelihood Funktion am Minimum gegeben. \n",
    "Gleichheit wird für spezielle Schätzverfahren oder im Grenzfall großer \n",
    "Stichproben erreicht; in der Praxis werden die Unsicherheiten aus den \n",
    "zweiten Ableitungen der negativen log-Liklihood nach den Paramtern unter\n",
    "Annahme des Gleichheitszeichens in obiger Beziehung bestimmt. \n",
    "Wenn die Zuverlässigkeit der so bestimmten Grenzen für die Parameterwerte\n",
    "wichtig ist, sollten Ensembletests mit einer großen Anzahl an Pseudodatensätzen \n",
    "durchgeführt werden, wie sie weiter unten beschrieben werden. \n",
    "\n",
    "Die Bestimmung der vollständigen Kovarianzmatrix der Parameterunsicherheiten \n",
    "ist wichtig, weil Korrelationen zwischen den Parametern auch bei unkorrelierten \n",
    "Messwerten sehr häufig auftreten. Wenn Korrelationen besonders groß sind, kann \n",
    "dies auch ein Hinweis auf eine ungeschickte Parametrisierung des Problems sein. \n",
    "Bei Korrelationen größer als 10% sollten sie als Teil des Ergebnisses unbedingt \n",
    "kommuniziert werden, weil die Form der innerhalb eines vorgegebenen \n",
    "Konfidenzniveaus noch zulässigen Parameterbereiche davon abhängt. \n",
    "\n",
    "Bei Problemstellungen, die in den Parametern linear sind, ist die\n",
    "log-Likelihood bei Vorliegen Gauß-förmiger Messunsicherheiten eine Parabel. \n",
    "Im Grenzfall großer Stichproben wird ebenfalls näherungsweise parabolisches\n",
    "Verhalten im Bereich um da Minimum erreicht. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grafische Darstellung\n",
    "\n",
    "Für unser erstes Beispiel wollen wir zur Illustration den Verlauf von \n",
    "$n2l{\\cal L}$ grafisch darstellen. In unserem einfachen Fall mit nur\n",
    "zwei freien Parametern stellt dies bereits die Lösung des Minimierungsproblems\n",
    "dar \n",
    "\n",
    "In der Codezelle unten ist dafür eine nützliche Hilfsfunktionen definiert.\n",
    "Die Codierung der Kostenfunktion muss zu deren Verwendung noch angepasst \n",
    "werden, um eine optimale vektorbasierte Berechnung für Arrays von \n",
    "Eingabeparametern zu ermöglichen.\n",
    "Für die Minimierung mit *iminuit* wurde die Kostenfunktion jeweils für\n",
    "einzelnen Parameterwerte berechnet - zur grafischen Darstellung wollen wir\n",
    "sie aber effizient in einem einzigen Aufruf auf einem zweidimensionalen \n",
    "Gitternetz auswerten. \n",
    "\n",
    "Die für diesen Fall angepasste Kodierung der Kostenfunktion ist ebenfalls \n",
    "in der Codezelle unten gezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def get_3Dfunction_data(xmin=1.,xmax=1.,ymin=-1.,ymax=1., \n",
    "                        func=None, fkwargs={}, delta=0.1):\n",
    "  '''\n",
    "    get function values on 2D-grid\n",
    "\n",
    "    Args: \n",
    "      float xmin: minimum x-range\n",
    "      float xmax: maximum x-range\n",
    "      float ymin: minimum y-range\n",
    "      float ymys: maximum y-range\n",
    "      funtion func: funtion f(x,y **kwargs)\n",
    "      dict fkwargs: prameters to pass to function\n",
    "      delta: resolution of grid\n",
    "\n",
    "    Returns: \n",
    "      tuple X, Y, Z\n",
    "  '''    \n",
    "  x = np.arange(xmin, xmax, delta)\n",
    "  y = np.arange(ymin, ymax, delta)\n",
    "  X, Y = np.meshgrid(x, y)\n",
    "  Z = func(X, Y, **fkwargs)\n",
    "  return X, Y, Z\n",
    "\n",
    "\n",
    "# define efficient version of cost function for arrays mu and sigma: \n",
    "#    2 * negative log likelihood of Gauß;\n",
    "def plotEfficient_myCost(mu, sigma):\n",
    "  # simple -2*log-likelihood of a 1-d Gauss distribution\n",
    "  n2lL = np.zeros_like(mu)\n",
    "  for i in range(len(data)):  \n",
    "    n2lL += (data[i]-mu)*(data[i]-mu)\n",
    "  n2lL /= sigma*sigma \n",
    "  return n2lL + 2.*len(data)*np.log(sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den besten quantitativen Überblick über eine skalare Funktion von zwei\n",
    "Variablen erhält man, wenn eine Konturdarstellung, analog zu Höhenlinien\n",
    "in der Geografie, gewählt wird. Hier der Code zur Erzeugung der Kontur-Grafik:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost function near minimum\n",
    "# - ranges close to minimum\n",
    "minx = 1.9\n",
    "maxx = 2.2\n",
    "miny = 0.4\n",
    "maxy = 0.64\n",
    "# get Z data for grid\n",
    "X, Y, Z = get_3Dfunction_data(xmin=minx, xmax=maxx, ymin=miny, ymax=maxy,\n",
    "                              func=plotEfficient_myCost, delta=0.005)\n",
    "minz = np.amin(Z)\n",
    "maxz = np.amax(Z)\n",
    "\n",
    "# create figure ...\n",
    "fig = plt.figure(figsize=(10., 7.5))\n",
    "# ... plot (filled) contour lines ...\n",
    "cont = plt.contourf(X, Y, Z, cmap='Blues', levels=25)\n",
    "# ... and add a legend \n",
    "cbar=plt.colorbar(cont, shrink=0.5, aspect=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Methode zur Bestimmung der Parameterunsicherheiten: Profil-Likelihood\n",
    "\n",
    "Bei nicht Gauß-förmigen Unsicherheiten der Messdaten und kleinen Stichproben \n",
    "ist der Verlauf am Minimum nicht mehr gut durch eine Parabel anzunähern, und \n",
    "die Angabe eines aus den zweiten Ableitungen am Punkt des Minimums beruhenden\n",
    "symmetrischen Intervalls für die Unsicherheiten ist dann häufig nicht mehr \n",
    "ausreichend. \n",
    "In solchen Fällen verwendet man zur Abschätzung des Vertrauensintervalls der Parameter \n",
    "einen Scan der sog. **Profil-Likelihood**. \n",
    "Bei diesem Verfahren wird für jeden Parameter eine Anzahl von dicht in der Nähe des\n",
    "Minimums liegenden Punkten betrachtet und die log-Likelihood bezüglich aller übrigen \n",
    "Parameter minimiert. Auf diese Weise werden insbesondere auch die Korrelationen mit\n",
    "anderen Parametern berücksichtigt. Das Vertrauensintervall, das dem Bereich plus/minus \n",
    "einer Standardabweichung einer Gaußverteilung um den Parameterwert, also\n",
    "einem Konfidenzniveau von 68.3% entspricht, erhält man an den Stellen,\n",
    "an denen die log-Likelihood um den Wert $\\frac{1}{2}$ größer ist als\n",
    "am Minimum (bzw. um Eins größer, wenn man $-2\\ln(\\cal L)$ verwendet). \n",
    "Für parabolische Verläufe ist das Verfahren zur auf den zweiten \n",
    "Ableitungen basierenden Methode äquivalent; bei nicht-parabolische Verläufen \n",
    "ergibt sich aus der Analyse der Profil-Likelihood ein asymmetrisches Konfidenzintervall\n",
    "um den Parameterwert am Minimum, das in solchen Fällen an Stelle der aus den zweiten \n",
    "Ableitungen am Minimum bestimmen Schätzungen der Standardabweichungen der Parameter\n",
    "angegeben werden sollte.\n",
    "\n",
    "Das Prinzip der Profil-Likelihood lässt sich auch in zwei Dimensionen,\n",
    "also für Paare von Parametern, anwenden. Man erhält dann Konturlinien,\n",
    "die Konfidenzbereichen analog zu ein- oder auch zwei-$\\sigma$-Konturen von \n",
    "Gauß-förmigen Verteilungen entsprechen. Bei starker Abweichung der Konturen \n",
    "von der Ellipsenform sollten sie zusammen mit dem Ergebnis berichtet werden.\n",
    "\n",
    "Profile-Likelihood-Kurven und Konfidenzkonturen lassen sich mit dem oben \n",
    "gezeigten Code leicht erzeugen, wenn die zusätzliche Option `plot_cor=True`\n",
    "angegeben wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_results = mFit(myCost, plot_cor=True)\n",
    "\n",
    "# Print results\n",
    "pvals, perrs, cor, gof, pnams = fit_results.values()\n",
    "print('\\n*==* user-defined cost: Fit Result:')\n",
    "print(\" parameter names:       \", pnams)\n",
    "print(\" parameter values:      \", pvals)\n",
    "print(\" neg. parameter errors: \", perrs[:,0])\n",
    "print(\" pos. parameter errors: \", perrs[:,1])\n",
    "print(\" correlations : \\n\", cor)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den mit dem Code oben erzeugten Matrix aus Grafiken werden auf der Diagonalen die\n",
    "Verläufe der Profil-Likelihood angezeigt; in den unteren Nebendiagonalelementen sind\n",
    "die 1- und 2-$\\sigma$ Konturen dargestellt. Die Fehlerbalken sind die aus der Analyse\n",
    "der zweiten Ableitungen gewonnenen Unsicherheiten, die in diesem Fall gut mit \n",
    "den mit Hilfe der Profil-Likelihood bestimmten Konfidenzintervallen übereinstimmen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel 2: log-Likelihood Anpassung einer Exponentialfuntion\n",
    "\n",
    "Wir wollen nun ein etwas komplexeres, realistisches Beispiel zur Anpassung von\n",
    "Parametern an Daten mit Hilfe des Maximum-Likelihood Verfahrens anschauen. \n",
    "\n",
    "Dazu verwenden wir echte Messungen der Lebensdauern von gestoppten \n",
    "kosmischen Myonen, die in einem Wasser-Cherenkov-Detektor (in diesem Fall \n",
    "einer Kaffeekanne mit Photomultiplier) nachgewiesen wurden. Beim Durchgang \n",
    "durch das Wasser erzeugen die Myonen ein Lichtsignal; wenn sie im Wasser\n",
    "oder im Boden darunter gestoppt werden, können Elektronen aus den zerfallenden\n",
    "Myonen wieder in den Detektor gelangen und nachgewiesen werden. Die Daten\n",
    "entsprechen den Zeitdifferenzen (in µs) solcher Doppelpulse. Zufällig\n",
    "eintreffende Myonen, Umgebungsstrahlung und evtl. Detektorrauschen erzeugen \n",
    "ebenfalls Doppelpulse, die im Gegensatz zu den Lebensdauern aber flach verteilt\n",
    "sind und einen Untergrund bei der Lebensdauermessung darstellen.\n",
    "\n",
    "Die relevante, auf Eins normierte Verteilungsdichte und die Daten finden \n",
    "sich in der folgenden Code-Zelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentialDecayPDF(t, tau=2., fbg=0.2, a=1., b=11.5):\n",
    "  \"\"\"Probability density function \n",
    "\n",
    "    for an exponential decay with flat background. The pdf is normed for the interval \n",
    "    [a=1µs,  b=11.5µs); these parameters a and b must be fixed in the fit! \n",
    "  \"\"\"\n",
    "\n",
    "  # 1.) Exponential distribution of life time measuremtens in the intervall [a, b]\n",
    "  pdf1 = np.exp(-t / tau) / tau / (np.exp(-a / tau) - np.exp(-b / tau))\n",
    "\n",
    "  # 2. flat distribution in the interval [a, b]\n",
    "  pdf2 = 1. / (b - a)\n",
    "\n",
    "# the full distribution is a combinatin of both, with a background fraction fbg   \n",
    "  return (1 - fbg) * pdf1 + fbg * pdf2\n",
    "\n",
    "\n",
    "# real data from measurement with a Water Cherenkov detector (\"Kamiokanne\")\n",
    "dT=[7.42, 3.773, 5.968, 4.924, 1.468, 4.664, 1.745, 2.144, 3.836, 3.132,\n",
    "    1.568, 2.352, 2.132, 9.381, 1.484, 1.181, 5.004, 3.06,  4.582, 2.076,\n",
    "    1.88,  1.337, 3.092, 2.265, 1.208, 2.753, 4.457, 3.499, 8.192, 5.101,\n",
    "    1.572, 5.152, 4.181, 3.52,  1.344, 10.29, 1.152, 2.348, 2.228, 2.172,\n",
    "    7.448, 1.108, 4.344, 2.042, 5.088, 1.02,  1.051, 1.987, 1.935, 3.773,\n",
    "    4.092, 1.628, 1.688, 4.502, 4.687, 6.755, 2.56,  1.208, 2.649, 1.012,\n",
    "    1.73,  2.164, 1.728, 4.646, 2.916, 1.101, 2.54,  1.02,  1.176, 4.716,\n",
    "    9.671, 1.692, 9.292, 10.72, 2.164, 2.084, 2.616, 1.584, 5.236, 3.663,\n",
    "    3.624, 1.051, 1.544, 1.496, 1.883, 1.92,  5.968, 5.89,  2.896, 2.76,\n",
    "    1.475, 2.644, 3.6,   5.324, 8.361, 3.052, 7.703, 3.83,  1.444, 1.343,\n",
    "    4.736, 8.7,   6.192, 5.796, 1.4,   3.392, 7.808, 6.344, 1.884, 2.332,\n",
    "    1.76,  4.344, 2.988, 7.44,  5.804, 9.5,   9.904, 3.196, 3.012, 6.056,\n",
    "    6.328, 9.064, 3.068, 9.352, 1.936, 1.08,  1.984, 1.792, 9.384, 10.15,\n",
    "    4.756, 1.52,  3.912, 1.712, 10.57, 5.304, 2.968, 9.632, 7.116, 1.212,\n",
    "    8.532, 3.000, 4.792, 2.512, 1.352, 2.168, 4.344, 1.316, 1.468, 1.152,\n",
    "    6.024, 3.272, 4.96, 10.16,  2.14,  2.856, 10.01, 1.232, 2.668, 9.176 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anpassung mit Hilfe des log-Likelihood-Verfahrens funktioniert genau so \n",
    "wie oben. Dieses Mal werden allerdings an Stelle der Kostenfunktion die Daten\n",
    "und die Verteilungsdichte sowie weitere Optionen zur grafischen Darstellung \n",
    "als Parameter angegeben. Damit die Anpassung funktioniert, muss das\n",
    "Anpassungswerkzeug einige besondere Optionen unterstützen: \n",
    "- die Begrenzung von Parametern auf einen sinnvollen Bereich   \n",
    "  (Option `limits=('fbg', 0., 1.)`) und \n",
    "- die Fixierung von Paramtern, die in der Anpassung nicht variiert werden. \n",
    "  In diesem Fall ist dies das Intervall, in dem die Messungen verlässlich sind\n",
    "  und auf das die Verteilungsdichte normiert ist      \n",
    "  (Option `fixPars = ['a', 'b']`).\n",
    "  \n",
    "Hier nun der vollständige Programmcode zur Anpassung einer Verteilungsdichte\n",
    "an (ungebinnte) Daten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mFit( exponentialDecayPDF,\n",
    "       data = dT, # data - if not None, a normalised PDF is assumed as model  \n",
    "       limits=('fbg', 0., 1.),  # parameter limits\n",
    "       fixPars = ['a', 'b'],    # fix parameter(s) \n",
    "       neg2logL = True,         # use  -2 * ln(L)\n",
    "       plot=True,               # plot data and model\n",
    "       plot_band=True,          # plot model confidence-band\n",
    "       plot_cor=True,          # plot profiles likelihood and contours\n",
    "       axis_labels=['life time  ' + '$\\Delta$t ($\\mu$s)', \n",
    "                       'Probability Density  pdf($\\Delta$t; *p)'], \n",
    "       data_legend = '$\\mu$ lifetime data',    \n",
    "       model_legend = 'exponential decay + flat background' )\n",
    "\n",
    "# Print results \n",
    "pvals, perrs, cor, gof, pnams = results.values()\n",
    "print('\\n*==* unbinned ML Fit Result:')\n",
    "print(\" parameter names:       \", pnams)\n",
    "print(\" parameter values:      \", pvals)\n",
    "print(\" neg. parameter errors: \", perrs[:,0])\n",
    "print(\" pos. parameter errors: \", perrs[:,1])\n",
    "print(\" correlations : \\n\", cor)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anders als bei der Anpassung der Gaußverteilung an Gauß-verteilte Daten zeigen die Profil-Likelihood\n",
    "und die Konturen in diesem Fall eine starke Abweichung von linearem Verhalten. \n",
    "In einem solchen Fall ist die Bereitstellung der Grafiken zusätzlich zur Angabe der numerischen Ergebnisse notwendig. Auch die stark asymmetrischen Unsicherheiten müssen unbedingt angegeben werden.  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anpassung von Verteilungsdichten an Histogramme\n",
    "***\n",
    "\n",
    "Ein weiteres typisches Beispiel für die Anwendung der log-Likelihood-Methode\n",
    "ist die Behandlung von Problemen, bei denen Poisson-verteilte Größen\n",
    "auftreten. Dies sind z.$\\,$B. die Anzahlen von Einträgen in einzelnen\n",
    "Intervallen von Häufigkeitsverteilungen (Histogramme).\n",
    "\n",
    "Die Poissonverteilung von Anzahlen $n_i$ mit Erwartungswerten $\\mu_i$\n",
    "ist gegeben durch\n",
    "\n",
    "\\begin{equation}\\label{equ-Poisson}\n",
    "  P(n_i;\\mu_i)=\\frac{{\\mu_i}^{n_i}} {n_i\\,!} \\, {\\rm e}^{-\\mu_i} \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Durch Bilden des Produkts über alle (als statistisch unabhängig\n",
    "angenommenen) $n_b$ Bins eines Histogramms erhält man die Likelihood \n",
    "\n",
    "\\begin{equation}\\label{equ-LPoisson}\n",
    "  {\\cal{L}}_{Poisson}=\\displaystyle\\prod_{i=1}^{n_b}\n",
    "  {\\rm P}\\left(n_i;\\mu_i(\\vec p)\\right)\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "und schließlich durch Logarithmieren und Weglassen des nicht von den \n",
    "Parametern abhängigen Terms $\\ln(n!)$ die negative log-Likelihood der\n",
    "Poisson-verteilten Anzahlen der Einträge in den Bins eines Histogramms\n",
    "\\begin{equation}\\label{equ-nlLPoisson}\n",
    "nl{\\cal{L}}_{Poisson}= -\\ln {\\cal{L}}_{Poisson} =\n",
    "\\displaystyle\\sum_{i=1}^{n_b} - n_i \\cdot \\ln(\\mu_i(\\vec p))\\,+\\,\\mu_i(\\vec p) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Bei Verwendung dieser Kostenfunktion im Minimierungsprozess können\n",
    "Anpassungen von Verteilungsdichten an Histogramme ganz analog wie bei\n",
    "der Verwendung der oben bereits diskutierten Likelihood zur Anpassung\n",
    "von Verteilungsdichten an Messdaten durchgeführt werden, die in diesem\n",
    "Fall aber als Histogramm vorliegen. \n",
    "Der Rechenaufwand hängt in diesem Beispiel von der Zahl der Bins und\n",
    "nicht - wie im ersten Beispiel - von der Größe des Datensatzes ab. \n",
    "\n",
    "Die Anzahl der auf Grund der Verteilungsdichte erwarteten Einträge $n_i$ \n",
    "bestimmt man sinnvollerweise durch Bildung des Integrals der Verteilung\n",
    "über die Binbreite - eine numerische Näherung (z.B. Simpson 2. Ordnung)\n",
    "ist meist ausreichend. \n",
    "\n",
    "In manchen Fällen wird auch eine Gauß-Näherung der Poisson-Verteilung\n",
    "verwendet,  $P(n_i;\\mu_i) = Gauss(n_i, \\mu_i, \\sigma=\\sqrt{\\mu_i})$.\n",
    "Bei großen Anzahlen an Bin-Einträgen ist diese Näherung akzeptabel und\n",
    "erlaubt häufig die Anwendung von Programmen zur Anpassung, die auf\n",
    "der Methode der kleinsten Fehlerquadrate beruhen und daher nur eine\n",
    "auf der Gaußverteilung beruhende Kostenfunktion erlauben. \n",
    "\n",
    "Wichtig ist es, die Schätzung der Unsicherheiten $\\sigma_i = \\sqrt{\\mu_i}$\n",
    "aus der Modellerwartung und nicht aus den beobachteten Werten in den\n",
    "Daten zu bestimmen, da letzteres zu einer verzerrten Parameterschätzung\n",
    "führen würde und Bins mit Null Einträgen sogar weggelassen werden müssten. \n",
    "Im Notfall kann man sich behelfen, indem zunächst eine Anpassung mit den aus\n",
    "der Beobachtung gewonnenen Unsicherheiten durchgeführt wird, die in \n",
    "einer wiederholten Anpassung durch die aus der dann näherungsweise bekannten \n",
    "Modellerwartung gewonnenen Werte ersetzt werden.\n",
    "Bei den empfohlenen Werkzeugen *kafe2* und *phyFit* sind solche Umstände\n",
    "allerdings nicht notwendig, da sie das korrekte, oben skizzierte \n",
    "Likelihood-Verfahren für die Anpassung an Histogrammdaten nutzen. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: Anpassung an Histogramm-Daten\n",
    "\n",
    "Die Anpassung eines Modells an histogrammierte Daten wird durch die Funktion\n",
    "*hFit()* aus dem Paket *PhyPraKit.phyFit* unterstützt. Die Vorgehensweise ist ganz\n",
    "analog zur Anpassung von ungebinnten Daten. Von Anwenderseite müssen lediglich\n",
    "die Daten und die Verteilungsdichte bereit gestellt sowie Angaben zu den \n",
    "Ausgabeoptionen gemacht werden. Die passenden Kostenfunktionen, also das zweifache\n",
    "des negativen natürlichen Logarithmus der Poissonverteilung - oder, oft in \n",
    "ausreichender Näherung der Gaußverteilung - sind in *hFit* implementiert. \n",
    "\n",
    "Hier das vollständige Progammbeispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhyPraKit.phyFit import hFit\n",
    "\n",
    "#  the model function to fit\n",
    "def SplusB_model(x, mu = 6.987, sigma = 0.5, s = 0.2):\n",
    "  '''pdf of a Gaussian signal on top of flat background\n",
    "  '''\n",
    "  normal = np.exp(-0.5*((x-mu)/sigma)**2)/np.sqrt(2.*np.pi*sigma**2)\n",
    "  linear = 1. / (xmx-xmn) \n",
    "  return s * normal + (1-s) * linear \n",
    "\n",
    "nbins=40\n",
    "xmn = 1\n",
    "xmx = 10\n",
    "bedges=np.linspace(xmn, xmx, nbins+1)\n",
    "bcontents = np.array([1, 1, 1, 2, 2, 2, 4, 1, 0, 3, 1, 1, 0,\n",
    "                        2, 3, 3, 1, 1, 0, 2, 3, 2, 3, 1, 1, 8,\n",
    "                        6, 7, 9, 1, 0, 1, 2, 1, 3, 2, 1, 3, 2, 4])\n",
    "#  \n",
    "# ---  perform fit  \n",
    "#\n",
    "results = hFit(SplusB_model,\n",
    "    bcontents, bedges,      # bin entries and bin edges\n",
    "    limits=[('s', 0., None), ('sigma', 0., None)], # parameter limits\n",
    "    plot=True,              # plot data and model\n",
    "    plot_band=True,         # plot model confidence-band\n",
    "    plot_cor=True,          # plot profiles likelihood and contours\n",
    "    axis_labels=['x', 'y   \\  f(x, *par)'], \n",
    "    data_legend = 'peudo-data',    \n",
    "    model_legend = 'signal + background model' )\n",
    "\n",
    "# Print results \n",
    "# pnams, pvals, perrs, cor, chi2 = results\n",
    "print('\\n*==* histogram fit Result:')\n",
    "print(\" parameter names:       \", results['parameter names'])\n",
    "print(\" goodness-of-fit: {:.3g}\".format(results['goodness-of-fit']))\n",
    "print(\" parameter values:      \", results['parameter values'])\n",
    "print(\" neg. parameter errors: \", results['confidence intervals'][:,0])\n",
    "print(\" pos. parameter errors: \", results['confidence intervals'][:,1])\n",
    "print(\" correlations : \\n\", results['correlation matrix'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die grafische Ausgabe zeigt die angepasste Verteilungsdichte sowie deren Unsicherheit, die durch \n",
    "Propagation der Paramerunsicherheiten auf die Modellvorhersage bestimmt wird. \n",
    "Die rechteckigen Flächen repräsentieren die Zahl der in den Daten beobachteten Einträge und \n",
    "entsprechen der klassischen Histogramm-Darstellung. Die um die Modellfunktion eingezeichneten\n",
    "Fehlerbalken zeigen den 68.3%-Konfidenzbereich für die unter Zugrundelegung der Poission-Verteilung\n",
    "erwartete Zahl an Einträgen in jedem Bin. Die Größe \"g.o.f\" (= \"goodness of fit\") ist die \n",
    "Differenz des beobachteten Werts von $n2l{\\cal L}$ und dem bestmöglichen Wert dieser Größe, \n",
    "den man erhält, wenn alle Daten auf der Modellvorhersage liegen, dem sog. \"Saturated Model\"; \n",
    "g.o.f. konvergiert für große Datensätze gegen die aus der Anpassung mit der Methode der kleinsten\n",
    "Fehlerquadrate bekannte Größe $\\chi^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein **Vergleich mit der Gauß-Näherung** ist durch Angabe einer weiteren Option\n",
    "`use_GaussApprox=True` beim Aufruf von *hFit* leicht möglich. Hier das gleiche\n",
    "Beispiel, bei dem im Vergleich zu vorhin nur diese Option geändert wurde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "# ---  perform 2nd Fit with Gaussian cost function  \n",
    "#\n",
    "results = hFit(SplusB_model,\n",
    "    bcontents, bedges,      # bin entries and bin edges\n",
    "    limits=[('s', 0., None), ('sigma', 0., None)], # parameter limits\n",
    "    use_GaussApprox=True,   # use Gaussian approxmiation of Poisson distr.\n",
    "    plot=True,              # plot data and model\n",
    "    plot_band=True,         # plot model confidence-band\n",
    "    plot_cor=True,          # plot profiles likelihood and contours\n",
    "    axis_labels=['x', 'y   \\  f(x, *par)'], \n",
    "    data_legend = 'peudo-data',    \n",
    "    model_legend = 's+b, Gauss Approx.' )\n",
    "\n",
    "# Print results \n",
    "# pnams, pvals, perrs, cor, chi2 = results\n",
    "print('\\n*==* histogram fit with Gaussian cost:')\n",
    "print(\" parameter names:       \", results['parameter names'])\n",
    "print(\" goodness-of-fit: {:.3g}\".format(results['goodness-of-fit']))\n",
    "print(\" parameter values:      \", results['parameter values'])\n",
    "print(\" neg. parameter errors: \", results['confidence intervals'][:,0])\n",
    "print(\" pos. parameter errors: \", results['confidence intervals'][:,1])\n",
    "print(\" correlations : \\n\", results['correlation matrix'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Maximum-Likelihood und Methode der kleinsten Fehlerquadrate\n",
    "***\n",
    "\n",
    "Die Methode der kleinsten Fehlerquadrate zur Anpassung von Modellen ist\n",
    "ein Spezialfall des maximum-Likelihood Verfahrens.\n",
    "\n",
    "Zunächst leiten wir ein auf der log-Likelihood Methode basierendes\n",
    "Anpassungsverfahren für Modellfunktionen an Messdaten her.\n",
    "Wir bezeichnen die zufällige Abweichung eines Messwertes vom wahren\n",
    "Wert mit $z$, die durch eine Verteilungsdichte $f_z$\n",
    "beschrieben wird. Im Falle von Messunsicherheiten ist $f_z$ häufig\n",
    "die Normalverteilung mit Erwartungswert Null und Standardabweichung $\\sigma$,\n",
    "${\\cal N}(z; \\sigma)$. Im mehrdimensionalen Fall für $n_d$ nicht\n",
    "notwendigerweise unabhängige Datenpunkte ist die multivariate\n",
    "Gaußverteilung ${\\cal N}(\\vec z; V)$ mit der Kovarianzmatrix $V$ relevant,\n",
    "\n",
    "\\begin{equation}\\label{equ-mStandardGauss}\n",
    "  {\\cal{N}}\\left(\\vec{z}, V) \\right) = \n",
    "\\frac{1} {\\sqrt{(2\\pi)^{n_d} \\det(V)} }\n",
    "\\cdot\n",
    "\\exp \\left( -\\frac{1}{2} \\vec{z}^T V^{-1} \\vec{z}\n",
    "\\right) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Die Zufallsgröße $z$ mit Erwartungswert Null entspricht den Fluktuationen \n",
    "um den wahren Wert. Wenn es für den wahren Wert eine exakte theoretische \n",
    "Erwartung in Form eines parameterbehafteten Modells $f_i(\\vec p)$ mit einem \n",
    "Satz an Parametern $\\vec p$ gibt, lässt sich ein Messwert $y_i$ schreiben als\n",
    "\\begin{equation}\\label{equ-zplusw}\n",
    "y_i =  f_i(\\vec p) + z_i \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Üblicherweise führt man zum Test von parameterbehafteten, durch\n",
    "Funktionen beschriebenen Modellen mehrere Messungen an verschiedenen\n",
    "Stützstellen $x_i=(\\vec x)_i$ durch, man betrachtet also eine Modellfunktion\n",
    "$f(\\vec x; \\vec p)$.\n",
    "Die Verteilungsdichte, die alle Messungen beschreibt, sieht dann so aus:\n",
    "\n",
    "\\begin{equation}\\label{equ-mGauss}\n",
    "  {\\cal{N}}\\left(\\vec{y}, V, f(\\vec{x}, \\vec{p}) \\right) = \n",
    "  \\frac{1} {\\sqrt{(2\\pi)^{n_d} \\det(V)} } \\cdot\n",
    "  \\exp\\left( -\\frac{1}{2} (\\vec{y}-\\vec{f})^T V^{-1} (\\vec{y}-\\vec{f}) \\right) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Nach dem Maximum-Likelihood-Prinzip ist der beste Parametersatz durch\n",
    "den Punkt $\\hat{\\vec{ p}}$ im Parameterraum gegeben, für den die Likelihood\n",
    "maximal wird. Wie schon oben verwendet man das Zweifache des negativen natürlichen\n",
    "Logarithmus der Likelihood, $n2l{\\cal L}_{Gauss}$, der dann minimiert wird.\n",
    "\\begin{equation}\\label{equ-nlLGauss}\n",
    "-2\\, \\ln{\\cal{L_{\\rm Gauß}}}\\left( \\vec y, V, \\vec{f}(\\vec x, \\vec {p}) \\right)\n",
    "\\,=\\, \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)^T V^{-1}\n",
    "  \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)\n",
    "  + \\ln(\\det(V)) + n_d\\,\\ln(2\\pi) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Für die Bestimmung des Minimums von $n2l{\\cal L}_{Gauss}$ sind bzgl. der\n",
    "Parameter konstante Terme nicht relevant, man kann sie daher weglassen.\n",
    "Wenn die Normierung der Gaußverteilung, also der Term $\\det(V)$, nicht\n",
    "von den Parametern abhängt, vereinfacht sich der obige Ausdruck\n",
    "zum altbekannten Ausdruck für die quadratische Residuensumme mit \n",
    "Kovarianzmatrix, $S$:\n",
    "\n",
    "\\begin{equation}\\label{equ-chi2}\n",
    "S\\left(\\vec y, V, \\vec{f}(\\vec x, \\vec {p})\\right) \\,=\\,\n",
    "  \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right)^T V^{-1}\n",
    "  \\left(\\vec y - \\vec f(\\vec x; \\vec p ) \\right) \\,.\n",
    "\\end{equation}\n",
    "\n",
    "Unter speziellen Bedingungen sind also die Minimierung von \n",
    "$n2l{\\cal{L}}$ und der quadratischen Summe der Residuen $S$ \n",
    "äquivalent. \n",
    "Dieser einfache Fall ist aber nicht mehr gegeben, wenn relative, \n",
    "auf den Modellwert bezogene Unsicherheiten auftreten, oder wenn \n",
    "Unsicherheiten in Abszissen-Richtung behandelt werden sollen, die \n",
    "mit Hilfe einer Taylor-Entwicklung erster Ordnung von der Abszisse \n",
    "auf die Ordinate übertragen werden und damit von der Ableitung des \n",
    "Modells nach $x$ und so auch von den Parameterwerten abhängen.\n",
    "Die praktische Regel lautet daher, möglichst immer die Likelihood\n",
    "zu verwenden, und nur in gut begründeten, berechtigten Fällen\n",
    "auf die Methode der kleinsten Fehlerquadrate zurück zu greifen.\n",
    "Allerdings ist man bei der Wahl der verfügbaren numerischen\n",
    "Werkzeuge stark eingeschränkt, wenn man diese Empfehlung\n",
    "umsetzen möchte.\n",
    "\n",
    "Sollen Problemstellungen mit nicht Gauß-förmigen Verteilungen\n",
    "behandelt werden, ist die Verwendung von log-Likelihoodverfahren\n",
    "unumgänglich. Das Aufstellen entsprechender Likelihood-Funktionen\n",
    "zur Behandlung spezieller Problemstellungen gehört in der\n",
    "wissenschaftlichen Praxis heute zum Standard. Dank sehr leistungsfähiger\n",
    "Algorithmen zur numerischen Minimierung in hoch-dimensionalen\n",
    "Parameterräumen und auch Dank moderner Computertechnik stellt\n",
    "die Verwendung korrekter Likelihood-Verfahren kein\n",
    "unüberwindliches Problem mehr dar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pratkische Hinweise\n",
    "\n",
    "In der Praxis sind die oben beschriebenen Verfahren nur in Kombination mit \n",
    "numerischen Verfahren zur Minimierung von skalaren Funktionen in mehr- oder\n",
    "sogar hoch-dimensionalen Räumen und Programmcode zur Verwaltung der Daten\n",
    "und ihrer Unsicherheiten durchführbar. Obwohl in seltenen Fällen auch analytische\n",
    "Lösungen existieren, nutzt man in der Praxis fast ausschließlich Programmpakete\n",
    "zur Durchführung von Anpassungen; analytische (Teil-)Lösungen müssen nur\n",
    "eingesetzt werden, wenn es um zeitkritische Problemstellungen geht. \n",
    "\n",
    "#### Konstruktion der Kovarianzmatrix\n",
    "\n",
    "Die Kovarianzmatrix der Daten bildet die Unsicherheiten Gauß-verteilter \n",
    "Eingabedaten vollständig ab. Wird sie bei der Anpassung berücksichtigt, \n",
    "werden alle zwischen den Daten unabhängigen und korrelierten Unsicherheiten\n",
    "beim Anpassungprozess in das Endergebnis propagiert - eine klassische \n",
    "Fehlerrechnung von Hand ist dann nicht mehr notwendig. \n",
    "\n",
    "Fassen wir zunächst kurz die wesentlichen Eigenschaften der\n",
    "Kovarianzmatrix ${\\bf V}$ zusammen:\n",
    "\n",
    "- ${\\bf V} = \\left(\\mathrm{V}_{ij}\\right)$ ist eine symmetrische Matrix;\n",
    "- sie hat die Dimension $n_d$, die der Anzahl der Messwerte entspricht;\n",
    "- für unabhängige Messwerte ist die Matrix diagonal;\n",
    "- die Nebendiagonalelemente $V_{ij},\\,{\\small i\\ne j}$ lassen sich verstehen als\n",
    "  das Produkt der gemeinsamen Unsicherheiten $\\sigma^g_i$ und $\\sigma^g_j$ der \n",
    "   Messungen $i$ und $j$;\n",
    "- die Kovarianzmatrix-Elemente für voneinander unabhängige Unsicherheiten\n",
    "  werden addiert.\n",
    "\n",
    "Gerade der letzte Punkt ist von entscheidender Bedeutung, denn er erlaubt es,\n",
    "die vollständige Kovarianzmatrix sukzessive aus einzelnen Beiträgen zur\n",
    "Unsicherheit aufzubauen, mit der Konstruktionsvorschrift:\n",
    "\n",
    "- Unsicherheiten der Messwerte werden nach verschiedenen, unabhängigen\n",
    "  Quellen aufgeschlüsselt.\n",
    "- Unabhängige Unsicherheiten jeder einzelnen Messung werden quadratisch in\n",
    "  den Diagonalelementen aufaddiert.\n",
    "- Allen Messwerten oder Gruppen von Messungen gemeinsame absolute oder\n",
    "  relative Unsicherheiten werden quadratisch in den betreffenden\n",
    "  Diagonal- und Nebendiagonalelementen $V_{ii}$, $V_{jj}$ und $V_{ij}$ aufaddiert.\n",
    "\n",
    "Unsicherheiten in Abszissenrichtung, gegeben durch eine Kovarianzmatrix\n",
    "${\\bf V}^x$, können berücksichtigt werden, in dem man mit sie Hilfe der\n",
    "Modellvorhersage $f(\\vec x, \\vec p)$ per Taylor-Entwicklung in erster Ordnung\n",
    "in y-Richtung transformiert und dann zur Kovarianz-Matrix der Datenpunkte in\n",
    "y-Richtung, ${\\bf V}^y$, addiert:\n",
    "\n",
    "\\begin{equation}\\label{equ-xyCovariance}\n",
    "  V_{ij} = (V^y)_{ij} \n",
    "        +  \\frac{\\partial f}{\\partial x_i} \\frac{\\partial f}{\\partial x_j} (V^x)_{ij} \n",
    "\\end{equation}\n",
    "\n",
    "Insgesamt ergeben sich so acht Arten von Unsicherheiten, nämlich \n",
    "unabhängige und / oder korrelierte\n",
    "absolute und / oder relative Unsicherheiten in\n",
    "$x$- und / oder $y$-Richtung.\n",
    "\n",
    "Zum Bau der Kovarianzmatrix setzt man idealerweise Programmcode ein. Die\n",
    "wenigsten gängigen Anpassungsprogramme bringen dafür direkt Optionen mit,\n",
    "sondern es muss die vollständige Kovarianzmatrix als Parameter übergeben\n",
    "werden.\n",
    "Das Paket *kafe2* enthält die Methode   \n",
    "`add_error(err_val, axis=?, correlation=?, relative=?, reference=?)`   \n",
    "mit deren Hilfe einzelne Komponenten der Unsicherheit hinzugefügt werden\n",
    "können. Die Interfaces zu verschiedenen Anpassungsprogrammen\n",
    "im Paket *PhyPraKit* sehen ebenfalls die Angabe einzelner Komponenten\n",
    "der Unsicherheit als Parameter vor, wenn die entsprechenden Anpassungspakete\n",
    "dies unterstützen.\n",
    "Alle acht Arten von Unsicherheiten können direkt nur mit *kafe2* oder \n",
    "mit *PhyPraKit.phyFit* behandelt werden. \n",
    "\n",
    "#### Berücksichtigung externer und eingeschränkter Parameter\n",
    "\n",
    "Häufig hängen Modellfunktionen von externen, mit Unsicherheiten behafteten \n",
    "Parametern ab, die z.$\\,$B. in einer Hilfsmessung bestimmt wurden oder aus der\n",
    "Literatur stammen. Dies können die Ergebnisse von Kalibrationsmessungen sein,\n",
    "oder auch Natur- oder Apparatekonstanten. Statt die Effekte der Parameterunsicherheiten \n",
    "mit Hilfe einer händischen Fehlerrechnung auf das Endergebnis zu propagieren, \n",
    "können  sie auch als eingeschränkte Parameter (\"constrained parameter\") direkt \n",
    "in der Anpassung berücksichtigt werden. \n",
    "Dazu werden solche Parameter gleichzeitig als freie Parameter in der \n",
    "Anpassung und als Messgrößen eingeführt - d.$\\,$h. ein entsprechender Term \n",
    "zur log-Likelihood hinzugefügt. \n",
    "\n",
    "Notwendig ist bisweilen auch eine Methode, mit deren Hilfe Parameter auf \n",
    "feste Werte fixiert werden können.\n",
    "Der Einfluss externer Parameter kann damit auch untersucht werden, indem\n",
    "man sie nacheinander auf ihren Erwartungswert und die jeweiligen oberen bzw. \n",
    "untere Grenzen ihres Konfidenzbereichs fixiert und die Veränderungen des \n",
    "Anpassungsergebnisses beobachtet.\n",
    "\n",
    "Das temporäre Fixieren und wieder frei geben von Parametern kann auch hilfreich \n",
    "sein, wenn eine komplexe Anpassung nicht zum globalen Minimum konvergiert. Man \n",
    "kann dann einen oder einige Parameter in der Nähe des erwarteten Wertes fixieren \n",
    "und eine Minimierung bezüglich der übrigen Parameter vornehmen. Wenn man dann an \n",
    "diesem temporären Minimum die fixierten Parameter wieder frei gibt, sollte \n",
    "die Anpassung zum globalen Minimum konvergieren. \n",
    "\n",
    "Ein Problem während des Anpassungsprozesses stellen oft auch Parameter dar,\n",
    "die temporär Werte in mathematisch nicht definierten Wertebereichen oder in \n",
    "unphysikalischen Bereichen annehmen (negative Werte unter Wurzeln, negative\n",
    "Massen etc). Das Setzen von Limits zum Ausschluss solcher Parameterbereiche\n",
    "ist eine wichtige Option für jedes Anpassungspaket. \n",
    "\n",
    "Bei nichtlinearen Problemstellungen gibt es häufig neben dem globalen Minimum\n",
    "weitere Nebenminima - oder sogar mehrere oder viele gleichwertige Lösungen. \n",
    "In solchen Fällen werden Startwerte für die Parameter in der Nähe\n",
    "einer \"vernünftigen\" Lösung benötigt. Auch diese Möglichkeit muss ein \n",
    "Anpassungspaket bieten. Da im Laufe des besseren Verständnisse aus manchem \n",
    "einfachen Problem später ein nichtlineares Problem werden kann, ist es eine \n",
    "gute Angewohnheit, grundsätzlich immer Startwerte zu setzen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Beispiel mit allen acht oben genannten Arten von Unsicherheiten ist in der Code-Zelle\n",
    "unten gezeigt. Alle in der Funkion *xyFit* verfügbaren Optionen sind ebenfalls angegeben,\n",
    "auch wenn für die meisten \"vernünftige\" Voreinstellungen gelten und man sich die explizite\n",
    "Angabe häufig ersparen kann. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel zur Anpassung an x/y-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from PhyPraKit.phyFit import mnFit, xyFit\n",
    "\n",
    "#\n",
    "# *** Example of an application of phyFit.mFit()\n",
    "#\n",
    "# define the model function to fit\n",
    "def exp_model(x, A=1., x0=1.):\n",
    "  return A*np.exp(-x/x0)\n",
    "\n",
    "# another model function\n",
    "def poly2_model(x, a=0.1, b=1., c=1.):\n",
    "  return a*x**2 + b*x + c\n",
    "\n",
    "# set model to use in fit\n",
    "#fitmodel=exp_model  # also try poly2_model !\n",
    "fitmodel=poly2_model  # also try exp_model!\n",
    "  \n",
    "# the data ...\n",
    "data_x = [0.0, 0.2, 0.4, 0.6, 0.8, 1., 1.2,\n",
    "          1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6]\n",
    "data_y = [1.149, 0.712, 0.803, 0.464, 0.398, 0.354, 0.148,\n",
    "          0.328, 0.181, 0.140, 0.065, 0.005,-0.005, 0.116]\n",
    "# ... and components of the uncertaity \n",
    "sabsy = 0.07 # independent y\n",
    "srely = 0.05 # 5% of model value\n",
    "cabsy = 0.04 # correlated\n",
    "crely = 0.03 # 3% of model value correlated\n",
    "sabsx = 0.05 # independent x\n",
    "srelx = 0.04 # 4% of x\n",
    "cabsx = 0.03 # correlated x\n",
    "crelx = 0.02 # 2% of x correlated\n",
    "\n",
    "# perform fit to data with function xyFit using class mnFit\n",
    "results = xyFit(fitmodel,      # the model function\n",
    "           data_x, data_y,     # x and y data\n",
    "           sx=sabsx,           # the error components defined above\n",
    "           sy=sabsy,\n",
    "           srelx=srelx,\n",
    "           srely=srely,\n",
    "           xabscor=cabsx,\n",
    "           xrelcor=crelx,\n",
    "           yabscor=cabsy,\n",
    "           yrelcor=crely,\n",
    "#           p0=(1., 0.5),     # start values of parameters (taken from model if not given)\n",
    "#           constraints=['A', 1., 0.03],    # constraint(s)\n",
    "#           constraints=[0, 1., 0.03]       # alternative specificaiton of constraints\n",
    "#           limits=('A', 0., None),  # parameter limits\n",
    "#           fixPars = ['A'],         # parameter(s) to be fixed to start value \n",
    "           use_negLogL=True,         # use full n2lL (default)\n",
    "           plot=True,                # plot data and model \n",
    "           plot_band=True,           # plot model uncerctainty\n",
    "           plot_cor=True,            # plot profiels and contours\n",
    "           quiet=True,               # silent output it True\n",
    "           axis_labels=['x', 'y   \\  f(x, *par)'], \n",
    "           data_legend = 'pseudo data',    \n",
    "           model_legend = 'model')\n",
    "\n",
    "# Print results \n",
    "print('\\n*==* xyFit Result:')\n",
    "for key in results:\n",
    "  print(\"{}\\n\".format(key), results[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5. Beurteilung der Qualität von Anpassungsverfahren\n",
    "***\n",
    "\n",
    "An eine Parameterschätung werden bestimmte Anforderungen gestellt, die je nach gewähltem\n",
    "Verfahren, aber auch abhängig von der Größe einer Stichprobe unterschiedlich gut erfüllt\n",
    "sein könnnen. Da die Parameter $\\hat{p}_i$ und ihre Unsicherheiten aus zufälligen Größen, \n",
    "den Daten, bestimmt werden, sind sie auch Zufallsgrößen mit Erwartungswerten \n",
    "$E[\\hat{p}_i]$ und und Varianz $V[\\hat{p}_i]$. \n",
    "\n",
    "Eine gute Parameterschätzung sollte folgende Eigenschaften haben:\n",
    "\n",
    "  - konsistent: $\\displaystyle \\lim_{n \\to \\infty} \\hat{p}_i = p_i$\n",
    "  \n",
    "  - möglichst erwartungstreu schon bei kleinen Stichproben: $E[\\hat{p}_i] = p_i$   \n",
    "  \n",
    "  - möglichst unverzerrt: Verzerrung $b$ (=bias) $b_i=E[\\hat{p}_i] - p_i$ =0,   \n",
    "    für große Stichprobem muss wegen der geforderten Erwartungstreue gelten\n",
    "    $\\displaystyle \\lim_{n \\to \\infty} b = 0$\n",
    "  \n",
    "  - optimal: $V[\\hat{p}_i]$ minimal   \n",
    "  \n",
    "  - robust: unbeeinflusst von \"Ausreißern\" oder leichten Fehlmodellierungen der Daten\n",
    "  \n",
    "  - korrekte Überdeckungswahrscheinlichkeit (engl. \"coverage\"): \n",
    "    $W(p_i \\in [\\hat{p}_i - \\Delta p_i, \\hat{p}_i +\\Delta p_i])= 68.3\\%$  \n",
    "\n",
    "Die hier diskutierte Maximum-Likelihood Methode (MLE) ist optimal, allerdings eher nicht\n",
    "robust, weil falsche Annahmen über das Modell einen empfindlichen Einfluss auf die\n",
    "Parameterwerte haben können. MLE-Schätzungen sind in der Regel auch nicht unverzerrt; \n",
    "wenn nicht unbedingt der Punkt der besten Anpassung (\"Punktschätzung\"), sondern, wie \n",
    "häufig in der Physik ein möglichst korrektes Konfidenzintervall (\"Intervallschätzung\") \n",
    "für den Schätzwert von Belang ist, spielt das allerdings eine untergeordnete Rolle. \n",
    "Wie wir schon oben gesehen hatten, existiert für die Bestimmung der Unsicherheiten nur \n",
    "eine Ungleichung (Cramér-Rao-Fréchet Grenze). Wie gut bestimme Verfahren typischerweise\n",
    "nach den oben genannten Kriterien funktionieren, ist oft empirisch bekannt. Mann kann\n",
    "zeigen, dass z.$\\,$B. die Methode der kleinsten Fehlerquadrate für lineare Problemstellungen\n",
    "und Gauß-förmige Messunsicherheiten optimal und unverzerrt ist. Die hier skizzierten\n",
    "Likelihood-Verfahren erfüllen die oben genannten Bedingungen schon für recht kleine \n",
    "Stichproben ebenfalls gut. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Überprüfung mittels Monte-Carlo - Methode (\"toy-MC\")\n",
    "\n",
    "Zur Überprüfung der Eigenschaften eines Schätzverfahrens und des Einflusses der Stichprobengröße\n",
    "werden in der Praxis häufig Ensemble-Tests durchgeführt - d.h. man erzeugt zufällig eine große \n",
    "Anzahl gleichwertiger Stichproben und führt jeweils das Anpassungsverfahren durch.\n",
    "Aus einer statistischen Analyse der Ergebnisse lassen sich dann die Eigenschaften des\n",
    "Schätzverfahrens bestimmen.\n",
    "\n",
    "Zur Simulation der Stichproben verwendet man die Monte-Carlo Methode; weil die Simulation\n",
    "für diesen Zweck sehr einfach sein kann, nennt man solche Studien auch \"toy-Monte Carlo\".\n",
    "\n",
    "Beispielcode zur Erzeugung künstlicher \"Pseudo-Daten\" ist in den folgenden Codezellen gezeigt.\n",
    "Die durchgeführte Anpassung entspricht dem schon oben diskutierten Beispiel zur Anpassung\n",
    "einer Modellfunktion an x/y-Daten mit verschiedenen Typen von Unsicherheiten. \n",
    "\n",
    "\n",
    "Hier der Code-Teil zur Definition aller Parameter zur Datenerzeugung und zur Anpassung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhyPraKit import generateXYdata\n",
    "from PhyPraKit.phyFit import xyFit, get_functionSignature\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# define some models to test\n",
    "def exp_model(x, A=1., x0=1.):\n",
    "  return A*np.exp(-x/x0)\n",
    "\n",
    "# another model function\n",
    "def poly2_model(x, a=0.75, b=1., c=1.):\n",
    "  return a*x**2 + b*x + c\n",
    "\n",
    "# set model to use ...\n",
    "model = poly2_model\n",
    "#model = exp_model\n",
    "\n",
    "# ... and number of pseudo-experiments to perform\n",
    "Nexp = 200\n",
    "\n",
    "# parameters of pseudo-data\n",
    "nd=120   # number of data points\n",
    "xmin=0.  # x-range\n",
    "xmax=2.5\n",
    "data_x = np.linspace(xmin, xmax, nd)      # x of data points\n",
    "\n",
    "# extract model paramerters from model function\n",
    "mpardict = get_functionSignature(model)[1] # keyword arguments of model\n",
    "true_vals = np.asarray(list(mpardict.values()))\n",
    "npar=len(mpardict)\n",
    "parnams = list(mpardict.keys())\n",
    "\n",
    "# set uncertainties for x/y pseudo-data\n",
    "# - independent absolute uncertainties\n",
    "sabsx = 0.05 # 0.05\n",
    "sabsy = 0.07 # 0.07 \n",
    "# - indepentent relative uncertainties\n",
    "srelx = 0.04 # 4%\n",
    "srely = 0.03 # 5% of model value\n",
    "# correlated uncertainties in x  and y direction \n",
    "cabsx = 0.03 # 0.03\n",
    "crelx = 0.02 # 2%\n",
    "cabsy = 0.04 # 0.04\n",
    "crely = 0.03 # 3% of model value\n",
    "\n",
    "# calulate total uncertainties\n",
    "sigx = np.sqrt(sabsx * sabsx + (srelx * data_x)**2)\n",
    "sigy = np.sqrt(sabsy * sabsy + (srely*model(data_x, **mpardict))**2)\n",
    "\n",
    "\n",
    "# set keyword arguments for fit function xyFit\n",
    "kw_uncertainties = {  # - components of uncertainty \n",
    "       'sx':sabsx,             # indep x\n",
    "       'sy': sabsy,            # indel y\n",
    "       'srelx': srelx,         # indep. rel. x\n",
    "       'srely': srely,         # indep. rel. y\n",
    "       'xabscor': cabsx,       # correlated x\n",
    "       'xrelcor': crelx,       # correlated rel. x\n",
    "       'yabscor': cabsy,       # correlated y\n",
    "       'yrelcor': crely,       # correlated rel. y\n",
    "       }\n",
    "\n",
    "kw_options = {  # options for Fit\n",
    "            'ref_to_model': True,   # reference of rel. uncert. to model\n",
    "            'use_negLogL': True     # standard chi^2 if false\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Schleife zur Erzeugung der Daten, Modellanpassung und die Analyse der Ergebnisse \n",
    "ist in der folgenden Codezelle gezeigt. Für die  Analyse der individuellen Ergebnisse \n",
    "der Anpassung sind die folgenden Zeilen zuständig. Sie speichern die individuellen \n",
    "Ergebnisse, berechnen die $\\chi^2$-Wahrscheinlichkeit, die Verzerrung und eine Bool'sche \n",
    "Variable für die Auswertung der Überdeckung zur späteren statistischen Auswertung:\n",
    "\n",
    "```\n",
    " #  calculate chi2 probability\n",
    "    chi2prb = 1.- stats.chi2.cdf(chi2, nd-len(pvals))\n",
    "    c2prb.append(chi2prb)\n",
    "    \n",
    "    # analyze bias and coverage\n",
    "    for i in range(npar):\n",
    "      biases[i].append( pvals[i] - true_vals[i] )  # bias\n",
    "      # get parameter confidence interval (CI)\n",
    "      pmn = pvals[i] + perrs[i,0]\n",
    "      pmx = pvals[i] + perrs[i,1]\n",
    "      # coverage: count how often true value is in CI\n",
    "      if (true_vals[i] >= pmn and true_vals[i]<=pmx): N_coverage[i] +=1\n",
    "    else:                 \n",
    "      # simple progress bar\n",
    "      if (100*iexp)%Nexp==0: print('*', end='')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the loop part of the MC simulation\n",
    "\n",
    "def MC_loop():\n",
    "  global nfail, biases, c2prb, N_coverage\n",
    "  # initialize arrays for statistical analysis in loop\n",
    "  nfail = 0                          # counter for failed fits\n",
    "  biases = [[] for i in range(npar)] # bias\n",
    "  c2prb = []                         # chi^2 probatility\n",
    "  N_coverage = npar*[0]              # counters for coverage\n",
    "\n",
    "#\n",
    "# start toy-MC loop --------------------------------------------   \n",
    "#\n",
    "  print(\"generating data - patience please!\")    \n",
    "  for iexp in range(Nexp):\n",
    "    #generate a sample of peudo-data\n",
    "    np.random.seed(314159 + (iexp * 2718281)%100000)     # initialize random generator\n",
    "    xt, yt, data_y = generateXYdata(data_x, model, sigx, sigy,\n",
    "                                   xabscor=cabsx,\n",
    "                                   xrelcor=crelx,\n",
    "                                   yabscor=cabsy,\n",
    "                                   yrelcor=crely,\n",
    "                                   mpar=true_vals )\n",
    "    # perform fit to pseudo data \n",
    "    try:\n",
    "      plot = True if iexp < 1 else 0\n",
    "      result = xyFit(model, data_x, data_y,      # model & data\n",
    "         **kw_uncertainties, # all uncertainties \n",
    "         **kw_options,        # fit options\n",
    "         plot=plot,           # plot data and model\n",
    "         showplots=False      # call plt.show() in user code if False\n",
    "      )\n",
    "\n",
    "      # save fit results in local variables\n",
    "      pvals, perrs, cor, chi2, pnams = result.values() \n",
    "\n",
    "      if plot:\n",
    "        plt.suptitle('Result of fit {:d}'.format(iexp))\n",
    "\n",
    "      if iexp < 1:\n",
    "      # Print results to illustrate how to use output\n",
    "        np.set_printoptions(precision=6)\n",
    "        print('\\n*==*  Fit {:d} Result:'.format(iexp))\n",
    "        print(f\" chi2: {chi2:.4g}\")\n",
    "        print(f\" parameter names:  \", pnams)\n",
    "        print(f\" parameter values:  \", pvals)\n",
    "        np.set_printoptions(precision=3)\n",
    "        print(f\" parameter errors: \", perrs)\n",
    "        np.set_printoptions(precision=3)\n",
    "        print(f\" correlations : \\n\", cor)\n",
    "        np.set_printoptions(precision=8) # default output precision\n",
    "        print()\n",
    "      else:                 \n",
    "        # show simple progress bar\n",
    "        if (100*iexp)%Nexp==0: print('*', end='')\n",
    "                \n",
    "      #  calculate chi2 probability\n",
    "      chi2prb = 1.- stats.chi2.cdf(chi2, nd-len(pvals))\n",
    "      c2prb.append(chi2prb)\n",
    "    \n",
    "      # analyze bias and coverage\n",
    "      for i in range(npar):\n",
    "        biases[i].append( pvals[i] - true_vals[i] )  # bias\n",
    "        # get parameter confidence interval (CI)\n",
    "        pmn = pvals[i] + perrs[i,0]\n",
    "        pmx = pvals[i] + perrs[i,1]\n",
    "        # coverage: count how often true value is in CI\n",
    "        if (true_vals[i] >= pmn and true_vals[i]<=pmx): N_coverage[i] +=1\n",
    "\n",
    "    except Exception as e:\n",
    "      nfail +=1\n",
    "      print('!!! fit failed ', nfail)\n",
    "      print(e)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auswertung der Ergebnisse\n",
    "\n",
    "Die Code zur Analyse der in der Schleife aufgezeichneten Daten gibt zunächst die für\n",
    "jeden der Parameter beobachtete Verzerrung aus, zusammen mit deren durch die endliche\n",
    "Statistik bedingten Parameterunsicherheiten. Statistisch signifikant sind nur Verzerrungen, \n",
    "die deutlich größer als die Unsicherheit auf den Mittelwert der Abweichungnesind. Zur Bewertung\n",
    "der Relevanz muss man sie in Bezug auf die Unsicherheit der Parameterwerte, also der Breite\n",
    "der Verteilung der Abweichungen, setzen. Erst wenn sie eine bedeutsame Größe davon erreichen, \n",
    "typischerweise größer als 5% - 10%,  sollten die Verzerrungen angegeben werden. \n",
    "\n",
    "Als zweite Größe wird die Coverage ausgegeben, die dem Anteil der Fälle entspricht, in\n",
    "denen der wahre Parameterwert innerhalb des Unsicherheitsintervalls liegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run MC loop\n",
    "MC_loop()\n",
    "print('\\n\\n*==* sucessfully analyzed {:d} data sets'.format(Nexp-nfail))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result():\n",
    "  global nfail, biases, c2prb, N_coverage\n",
    "    \n",
    "  # - convert output to numpy arrays\n",
    "  for i in range(npar):\n",
    "    biases[i] = np.array(biases[i])\n",
    "  c2prb = np.array(c2prb)\n",
    "\n",
    "  ## plt.show() # blocks at this stage until figure deleted\n",
    "  # results as printout\n",
    "  N_succ = Nexp - nfail \n",
    "  print('\\n\\n*==* ', N_succ, ' successful fits done:')\n",
    "  print(' * parameter names:')\n",
    "  for i in range(npar):\n",
    "    print('   {:d}: {:s}'.format(i, pnams[i])) \n",
    "\n",
    "  print(' * biases:')\n",
    "  for i in range(npar):\n",
    "  #  bias = deviation of parameters from their true values\n",
    "    b = biases[i].mean()\n",
    "    e = biases[i].std()/np.sqrt(Nexp)\n",
    "    print('   {:d}: {:.3g} +\\- {:.2g}'.format(i, b, e)) \n",
    "\n",
    "  print(' * coverage:')\n",
    "  for i in range(npar):\n",
    "  #  coverage: fraction of true val in confidence interval\n",
    "    p_coverage = N_coverage[i]/(N_succ)*100//0.682689492\n",
    "    print('   {:d}: {:.3g}%'.format(i, p_coverage))\n",
    "\n",
    "print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispeil gibt es keine signifikanten Verzerrungen, und die Überdeckung liegt\n",
    "in der Nähe des für ein $\\pm 1\\sigma$-Intervall erwarteten Werts, d.h. 100% von 68.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grafische Darstellung der Ergebnisse\n",
    "\n",
    "Eine einfache grafische Darstellung der Ergebnisse zeigt das Code-Fragment unten. \n",
    "Verwendung findet eine allgemeine Funktion zur Darstellung von Verteilungen und Korrelationen \n",
    "eines Datensatzes mit mehreren Variablen aus dem Paket *PhyPraKit*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PhyPraKit import plotCorrelations  \n",
    "\n",
    "def plot_correlations():\n",
    "  global nfail, biases, c2prb, N_coverage\n",
    "  # plot results as array of sub-figures\n",
    "  names = [r'$\\Delta$'+pnams[i] for i in range(len(pnams))]\n",
    "  fig, ax = plotCorrelations(biases, names)\n",
    "  fig.suptitle(\"Biases and Correlations\", size='xx-large')\n",
    "  ax.text(0.1, 0.45,                \n",
    "        '$\\\\Delta$: fit - true \\n \\n' +\n",
    "        '$\\\\mu$: mean \\n' +\n",
    "        '$\\\\sigma$: standard deviation \\n' +\n",
    "        '$\\\\sigma_\\\\mu$: error on mean \\n \\n' +\n",
    "        '$\\\\rho$: correlation coefficient',\n",
    "         transform=ax.transAxes)\n",
    "  plt.show()\n",
    "  \n",
    "plot_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beobachteten Verteilungen bestätigen die Schätzungen der Unsicherheiten und der Korrelationen durch die in *xyFit()* verwendeten Methoden. Wenn sich deutliche Unterschiede ergeben hätten, würde man die aus der Ensemble-Studie abgeleiteten Unsicherheiten und Korrelationen als Ergebnis angeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verlässlichkeit des Hypothesentests\n",
    "\n",
    "Als letzes Beispiel ist unten die Überprüfung der Qualität der als Hypothesentest eingesetzten\n",
    "und aus dem in der Anpassung erhaltenen Wert von $\\chi^2$ am Optimum und der Zahl der Freiheitsgrade\n",
    "berechneten $\\chi^2$-Wahrscheinlichkeit gezeigt. Diese sollte flach sein, wenn die Größe zur Bestimmung\n",
    "der Goodness-of-Fit tatsächlich einer $\\chi^2$-Verteilung folgt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_chi2():\n",
    "  global nfail, biases, c2prb, N_coverage\n",
    "  # analysis of chi2 probability\n",
    "  figc2prb = plt.figure(figsize=(7.5, 5.))\n",
    "  ax = figc2prb.add_subplot(1,1,1)\n",
    "  nbins= int(min(50, Nexp/20))\n",
    "  binc, bine, _ = ax.hist(c2prb, bins=nbins, ec=\"grey\")\n",
    "  plt.xlabel('chi2 probability')\n",
    "  plt.ylabel('entries/bin')\n",
    "  # - check compatibility with flat distribution\n",
    "  mean = (Nexp-nfail)/nbins\n",
    "  chi2flat= np.sum( (binc - mean)**2 / mean )\n",
    "  prb = 1.- stats.chi2.cdf(chi2flat, nbins)\n",
    "  print('compatibility of chi2 probability with flat distribution: {:f}%'.format(prb*100))  \n",
    "\n",
    "  plt.show()\n",
    "\n",
    "analyse_chi2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall ist die beobachtete Verteilung tatsächlich im Rahmen eines ebenfalls mit der\n",
    "$\\chi^2$-Methode durchgeführten Tests auf Übereinstimmung mit einer Rechteckverteilung flach. Allerdings zeigen sich bei Erhöhung der Zahl der untersuchten Stichproben statistisch\n",
    "signifikante Abweichungen, die aber noch hinreichend klein sind, so dass man den Wert von\n",
    "$\\chi^2$ als Qualitätskriterium für die Anpassung verwenden kann. Mit Hilfe der nun\n",
    "aus dem Ensemble-Test bekannten Verteilung könnte man auch die kritische Grenze für das \n",
    "Verwerfen der Modellhypothese entsprechend angepassen. \n",
    "\n",
    "Übrigens: der Hauptgrund für die Abweichung der $\\chi^2$-Wahrscheinlichkeit von der \n",
    "Rechteckverteilung sind in diesem Beispiel die relativen, auf den Modellwert bezogenen\n",
    "Unsicherheiten der Messdaten. Schaltet man diese in der Simulation und im Modell aus,\n",
    "ergibt sich eine deutlich bessere Übereinstimmung mit der Rechteckverteilung. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Überprüfung der Robustheit\n",
    "\n",
    "Die Überprüfung der Robustheit einer Anpassung ist eines der schwierigsten Probleme. \n",
    "Auch hier helfen Ensemble-Tests, bei denen dann aber Stichproben der Daten erzeugt werden,\n",
    "die anders aussehen als im Modell angenommen. So können Effekte von falsch abgeschätzten\n",
    "Unsicherheiten, leicht variierten Modellfunktionen oder von seltenen Ausreißern in \n",
    "Verteilungen studiert werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studie 1: Behandlung relativer Unsicherheiten\n",
    "\n",
    "Mit dem oben definierten Set-up sind nun leicht weitere Studien der Auswirkungen bestimmter \n",
    "Optionen der Anpassung leicht durchzuführen. Dazu müssen lediglich die entsprechenden Optionen\n",
    "verändert und die Sequenz der oben durchgeführten Schritte wiederholt werden. \n",
    "Gezeigt ist das im Beispiel in der nächsten Zelle für den Fall, dass relative Unsicherheiten auf die \n",
    "Daten statt auf den Modellwert bezogen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify fit optoins\n",
    "kw_options = {  # options for Fit\n",
    "            'ref_to_model': False,   # !!! rel. uncert. w.r.t to data\n",
    "            'use_negLogL': True     # standard chi^2 if false\n",
    "       }\n",
    "# execute MC loop and analysis ...\n",
    "MC_loop()\n",
    "# ... and print results\n",
    "print('\\n\\n*==* sucessfully analyzed {:d} data sets'.format(Nexp-nfail))  \n",
    "print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie der Output zeigt, sind die Parameter nun signifkant verzerrt, und die Überdeckung\n",
    "ist sehr viel schechter geworden, d.h. die Angaben der Unsicherheiten sind in diesem \n",
    "Fall nicht zuverlässig.  Die grafische Darstellung der Ergebnisse des ersten Pseudodatensatzes\n",
    "zeigt, was passiert: Datenpunkten, die zu kleineren Werten fluktuieren, werden kleinere \n",
    "relative Unsicherheiten zugewiesen, und dementsprechend größere Unsicherheiten für Datenpunkte,\n",
    "die zu größeren Werten fluktuieren. Im Ergebnis wird die angepasste Modellfunktion dadurch hin zu\n",
    "kleineren $y$-Werten gezogen.  Nicht in allen Fällen ist der Effekt so deutlich in der Ergebnisgrafik\n",
    "sichtbar wie indiesem Fall! Den größten Beitrag zur offensichtlichen Abweichung des angepassten\n",
    "Modells an die Daten liefert übrigens die relative, korrelierte Unsicherheit von 3% auf die $y$-Werte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Studie 2: Vergleich mit der Methode der kleinsten Quadrate\n",
    "\n",
    "Verwendet man anstelle der Likelihood-Methode die klassische $\\chi^2$-Methode, so ergeben \n",
    "sich ebenfalls starke Verzerrungen und schlechte Werte für die Abdeckung des Konfidenzintervalls\n",
    "der Parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify fit optoins\n",
    "kw_options = {  # options for Fit\n",
    "            'ref_to_model': True,   # rel. uncert. w.r.t model\n",
    "            'use_negLogL': False     # !!! use standard chi^2 \n",
    "       }\n",
    "# execute MC loop and analysis ...\n",
    "MC_loop()\n",
    "print('\\n\\n*==* sucessfully analyzed {:d} data sets'.format(Nexp-nfail))  \n",
    "# ... and print results\n",
    "print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall wird die angepasst Modellfuktion hin zu größeren $y$-Werten gezogen. Der Grund\n",
    "ist die Vernachlässigung der parameterabhängigen Unsicherheiten in der Kostenfunkton. \n",
    "Wenn die Modellfunktion größere Werte vorhersagt, werden auch die relativen Unsicherheiten \n",
    "entsprechend größer und damit der $\\chi^2$-Wert entsprechend kleiner. Der Term $\\ln(\\det(V))$ \n",
    "in der vollen log-Likelihood wirkt diesem Effekt entgegen. Wieder ist in vielen Fällen das Problem\n",
    "nicht so klar sichtbar wie in diesem Beispeil mit recht großen Anteilen relativer Unsicherheiten.\n",
    "Der Test \"mit dem Auge\", d.h. scheinbar gute Übereinstimmung von Datenpunkten und Modell, \n",
    "täuscht oft.\n",
    "\n",
    "Im Ergebnis ist festzuhalten, dass unbedingt die Optionen             \n",
    "```\n",
    "ref_to_model=True,     # rel. uncert. w.r.t model\n",
    "use_negLogL': True     # use standard n2lL cost function\n",
    "```\n",
    "verwendet werden sollten. In *kafe* und *PhyPraKit.phyFit* ist das korrekte Verhalten\n",
    "voreingestellt. Bei Verwendung anderer Anpassungsprogramme ist Vorsicht geboten,\n",
    "wenn relative Unsicherheiten im Spiel sind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Abschließende Anmerkungen\n",
    "\n",
    "Mit den in diesem Tutorial beschriebenen Verfahren zur Anpassung von Modellen an Messdaten\n",
    "sind Sie für Anwendungen in den Fortgeschrittenenpraktika oder in Ihren Abschlussarbeiten\n",
    "bestens gerüstet. Die beschriebenen Methoden entsprechen den in der Wissenschaft üblichen,\n",
    "und Sie werden in wissenschaftlichen Veröffentlichungen einigen davon wieder begegnen. \n",
    "\n",
    "Die hier gezeigten Beispiele basieren mit Absicht auf dem schlanken Anpassungswerkzeug \n",
    "*phyFit*, um die Transparenz der Vorgehensweise sicher zu stellen und Ihnen durch \n",
    "Modifikation der Beispiele eigene Studien und Anpassungen an spezielle Problemstellungen\n",
    "zu ermöglichen. Da *phyFit* nur eine schlanke Interface-Ebene darstellt und im Übrigen auf dem \n",
    "im wissenschaftlichen Umfeld seit langem etablierten und anerkannten Paket MINUIT zur Optimierung\n",
    "in hochdimensionalen Parameterräumen und zur Schätzung der  Parameterunsicherheiten basiert, sind \n",
    "Ihre damit erzielten Ergebnisse fachlich sehr gut abgesichert und in eigenen Arbeiten \"zitierfähig\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
